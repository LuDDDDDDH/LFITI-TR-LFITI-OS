{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7806dcfc-b7e6-4aa9-bc31-29e25dbe7c2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature selection and model selection--------------------------------radiomics model selection\n",
    "\"\"\"\n",
    "This file demonstrates a survival analysis pipeline for preliminary model selection.\n",
    "It contains:\n",
    "  1. Data loading & preprocessing.\n",
    "  2. Base model tuning and training with:\n",
    "       - Random Survival Forest (RSF)\n",
    "       - Gradient Boosting Survival Analysis (GBSA)\n",
    "       - FastSurvivalSVM\n",
    "  3. Cross-validation for base models (without ensemble methods).\n",
    "  4. Calculation of the Brier score for a model.\n",
    "  \n",
    "Required packages:\n",
    "  - scikit-survival, lifelines, numpy, pandas, matplotlib, scikit-learn, shap\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency, spearmanr\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV, LinearRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "\n",
    "def custom_brier_score(y_true, y_pred, time_points):\n",
    "    \"\"\"\n",
    "    Compute custom Brier score for survival prediction evaluation.\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    brier_scores = []\n",
    "    for t in time_points:\n",
    "        error_sum = 0.0\n",
    "        for i in range(n_samples):\n",
    "            event_time = y_true[i][\"lenfol\"]\n",
    "            idx = np.where(np.isclose(time_points, t))[0]\n",
    "            if len(idx) == 0:\n",
    "                continue\n",
    "            prob_survival = y_pred[i][idx[0]]\n",
    "            if event_time > t:\n",
    "                error = (1 - prob_survival)**2\n",
    "            else:\n",
    "                error = (prob_survival)**2\n",
    "            error_sum += error\n",
    "        brier_scores.append(error_sum / n_samples)\n",
    "    return np.array(brier_scores)\n",
    "\n",
    "def load_and_preprocess(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the survival data with detailed feature selection steps.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading and Preprocessing Data ===\")\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_excel(file_path)\n",
    "    data_x = data.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    data_y = np.array([(bool(status), float(time))\n",
    "                       for status, time in zip(data[\"event\"], data[\"time\"])],\n",
    "                      dtype=[(\"fstat\", \"?\"), (\"lenfol\", \"f8\")])\n",
    "    labels = data[\"label\"].values\n",
    "\n",
    "    # Display original feature count and names\n",
    "    # print(\"\\n=== Feature Selection Steps ===\")\n",
    "    # print(f\"1. Original number of features: {len(data_x.columns)}\")\n",
    "    # print(\"Original features:\")\n",
    "    # for feat in data_x.columns:\n",
    "    #     print(f\"   - {feat}\")\n",
    "\n",
    "    # One-hot encoding\n",
    "    from sksurv.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    x_encoded = encoder.fit_transform(data_x)\n",
    "    try:\n",
    "        feature_names = encoder.get_feature_names_out(data_x.columns)\n",
    "    except AttributeError:\n",
    "        feature_names = np.array(data_x.columns)\n",
    "    \n",
    "    print(f\"\\n2. After one-hot encoding: {x_encoded.shape[1]} features\")\n",
    "    \n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(x_encoded)\n",
    "    \n",
    "    # Variance threshold selection\n",
    "    vt = VarianceThreshold(threshold=0.1)\n",
    "    X_scaled = vt.fit_transform(X_scaled)\n",
    "    feature_names = feature_names[vt.get_support()]\n",
    "    print(f\"\\n3. After variance thresholding: {len(feature_names)} features\")\n",
    "    # print(\"Features retained after variance thresholding:\")\n",
    "    # for feat in feature_names:\n",
    "    #     print(f\"   - {feat}\")\n",
    "    \n",
    "    # Cox regression feature selection\n",
    "    from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "    cox_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alphas=[0.1])\n",
    "    try:\n",
    "        cox_model.fit(X_scaled, data_y)\n",
    "    except Exception as e:\n",
    "        print(f\"Cox model fitting error: {e}\")\n",
    "        return None, None, None, None, None, None, None, None, None, labels\n",
    "    \n",
    "    cox_coef = pd.Series(cox_model.coef_[:, 0], index=feature_names)\n",
    "    selected_features_cox = cox_coef[cox_coef != 0].index\n",
    "    print(f\"\\n4. After Cox regression: {len(selected_features_cox)} features\")\n",
    "    print(\"Features selected by Cox regression:\")\n",
    "    for feat in selected_features_cox:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    # ElasticNet selection\n",
    "    elastic_net = ElasticNetCV(cv=5, random_state=42)\n",
    "    X_train_selected_cox = X_scaled[:, np.isin(feature_names, selected_features_cox)]\n",
    "    elastic_net.fit(X_train_selected_cox, data_y[\"lenfol\"])\n",
    "    coef_abs = np.abs(elastic_net.coef_)\n",
    "    top_10_indices = np.argsort(coef_abs)[-10:]\n",
    "    selected_features_elasticnet = selected_features_cox[top_10_indices]\n",
    "    \n",
    "    print(f\"\\n5. Final features after ElasticNetCV: {len(selected_features_elasticnet)} features\")\n",
    "    print(\"Final selected features (ranked by importance):\")\n",
    "    for i, feat in enumerate(selected_features_elasticnet, 1):\n",
    "        coef = elastic_net.coef_[top_10_indices[i-1]]\n",
    "        print(f\"   {i}. {feat} (coefficient: {coef:.4f})\")\n",
    "    \n",
    "    # Save feature selection results\n",
    "    feature_selection_results = pd.DataFrame({\n",
    "        'Feature': selected_features_elasticnet,\n",
    "        'Coefficient': elastic_net.coef_[top_10_indices]\n",
    "    })\n",
    "    feature_selection_results = feature_selection_results.sort_values('Coefficient', ascending=False)\n",
    "    output_file = 'feature_selection_results.xlsx'\n",
    "    feature_selection_results.to_excel(output_file, index=False)\n",
    "    print(f\"\\nFeature selection results have been saved to: {output_file}\")\n",
    "\n",
    "    # Process final selected features\n",
    "    final_feature_indices = np.isin(feature_names, selected_features_elasticnet)\n",
    "    X_final = X_scaled[:, final_feature_indices]\n",
    "    selected_features = feature_names[final_feature_indices]\n",
    "    \n",
    "    # Dataset split\n",
    "    X_train, X_test, y_train, y_test, labels_train, labels_test = train_test_split(\n",
    "        X_final, data_y, labels, test_size=0.3, random_state=42, stratify=data_y[\"fstat\"]\n",
    "    )\n",
    "    \n",
    "    # Adjust follow-up time\n",
    "    original_test_max = np.max(y_test[\"lenfol\"])\n",
    "    new_max = original_test_max * 0.99\n",
    "    y_train_adj = y_train.copy()\n",
    "    y_test_adj = y_test.copy()\n",
    "    y_train_adj[\"lenfol\"][y_train_adj[\"lenfol\"] >= original_test_max] = new_max\n",
    "    y_test_adj[\"lenfol\"][y_test_adj[\"lenfol\"] >= original_test_max] = new_max\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    min_test = np.min(y_test_adj[\"lenfol\"])\n",
    "    time_points = np.linspace(min_test, new_max - epsilon, num=75)\n",
    "    \n",
    "    return (X_train, X_test, y_train_adj, y_test_adj, selected_features,\n",
    "            time_points, X_scaled, data_y, feature_names, labels_train, labels_test)\n",
    "\n",
    "def compute_cv_cindices(X_train, y_train, X_test, y_test, cv=5):\n",
    "    \"\"\"\n",
    "    Compute cross-validated concordance indices for all models.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    rsf_param_grid = {\n",
    "        'n_estimators': [100, 500],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [5, 10]\n",
    "    }\n",
    "    gbsa_param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.5, 1.0]\n",
    "    }\n",
    "    svm_param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "    }\n",
    "    \n",
    "    cv_scores = {\n",
    "        \"RSF\": [],\n",
    "        \"GBSA\": [],\n",
    "        \"FastSurvivalSVM\": []\n",
    "    }\n",
    "    \n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # RSF\n",
    "        rsf_grid = GridSearchCV(RandomSurvivalForest(random_state=42),\n",
    "                              param_grid=rsf_param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        rsf_grid.fit(X_tr, y_tr)\n",
    "        best_rsf = rsf_grid.best_estimator_\n",
    "        pred_rsf = best_rsf.predict(X_test)\n",
    "        cindex_rsf = concordance_index_censored(y_test[\"fstat\"], y_test[\"lenfol\"], pred_rsf)[0]\n",
    "        cv_scores[\"RSF\"].append(cindex_rsf)\n",
    "        \n",
    "        # GBSA\n",
    "        gbsa_grid = GridSearchCV(GradientBoostingSurvivalAnalysis(random_state=42),\n",
    "                               param_grid=gbsa_param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        gbsa_grid.fit(X_tr, y_tr)\n",
    "        best_gbsa = gbsa_grid.best_estimator_\n",
    "        pred_gbsa = best_gbsa.predict(X_test)\n",
    "        cindex_gbsa = concordance_index_censored(y_test[\"fstat\"], y_test[\"lenfol\"], pred_gbsa)[0]\n",
    "        cv_scores[\"GBSA\"].append(cindex_gbsa)\n",
    "        \n",
    "        # FastSurvivalSVM\n",
    "        svm_grid = GridSearchCV(FastSurvivalSVM(random_state=42),\n",
    "                              param_grid=svm_param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        svm_grid.fit(X_tr, y_tr)\n",
    "        best_svm = svm_grid.best_estimator_\n",
    "        pred_svm = best_svm.predict(X_test)\n",
    "        cindex_svm = concordance_index_censored(y_test[\"fstat\"], y_test[\"lenfol\"], pred_svm)[0]\n",
    "        cv_scores[\"FastSurvivalSVM\"].append(cindex_svm)\n",
    "        \n",
    "        print(f\"Fold {fold} complete.\")\n",
    "        fold += 1\n",
    "\n",
    "    avg_scores = {method: np.mean(cv_scores[method]) for method in cv_scores}\n",
    "    return avg_scores\n",
    "\n",
    "def print_session_info():\n",
    "    \"\"\"Print session information, including date/time and user login info\"\"\"\n",
    "    current_time = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    current_user = os.getenv('USERNAME', 'Unknown')\n",
    "    \n",
    "    print(\"=== Session Information ===\")\n",
    "    print(f\"Date and Time (UTC): {current_time}\")\n",
    "    print(f\"User Login: {current_user}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the survival analysis pipeline.\n",
    "    \"\"\"\n",
    "    print_session_info()\n",
    "    \n",
    "    file_path = \"xxxxxxxxxxxxxxxxxxx.xlsx\"\n",
    "    print(f\"Processing file: {file_path}\\n\")\n",
    "    \n",
    "    res = load_and_preprocess(file_path)\n",
    "    if res is None:\n",
    "        print(\"Data loading failed, exiting.\")\n",
    "        return\n",
    "        \n",
    "    (X_train, X_test, y_train, y_test, selected_features,\n",
    "     time_points, X_scaled, data_y, feature_names_all, labels_train, labels_test) = res\n",
    "\n",
    "    # Base model tuning and training\n",
    "    print(\"\\n=== Model Tuning and Training ===\")\n",
    "    \n",
    "    print(\"\\n--- Tuning RSF ---\")\n",
    "    rsf_param_grid = {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'min_samples_split': [5, 10, 15],\n",
    "        'min_samples_leaf': [5, 10, 15]\n",
    "    }\n",
    "    rsf_grid = GridSearchCV(RandomSurvivalForest(random_state=42),\n",
    "                          param_grid=rsf_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    rsf_grid.fit(X_train, y_train)\n",
    "    best_rsf = rsf_grid.best_estimator_\n",
    "    print(\"Best RSF parameters:\", rsf_grid.best_params_)\n",
    "    risk_scores_rsf_train = best_rsf.predict(X_train)\n",
    "    risk_scores_rsf_test = best_rsf.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- Tuning GBSA ---\")\n",
    "    gbsa_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.5, 0.8, 1.0]\n",
    "    }\n",
    "    gbsa_grid = GridSearchCV(GradientBoostingSurvivalAnalysis(random_state=42),\n",
    "                           param_grid=gbsa_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    gbsa_grid.fit(X_train, y_train)\n",
    "    best_gbsa = gbsa_grid.best_estimator_\n",
    "    print(\"Best GBSA parameters:\", gbsa_grid.best_params_)\n",
    "    risk_scores_gbsa_train = best_gbsa.predict(X_train)\n",
    "    risk_scores_gbsa_test = best_gbsa.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- Tuning FastSurvivalSVM ---\")\n",
    "    svm_param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
    "    svm_grid = GridSearchCV(FastSurvivalSVM(random_state=42),\n",
    "                          param_grid=svm_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    svm_grid.fit(X_train, y_train)\n",
    "    best_svm = svm_grid.best_estimator_\n",
    "    print(\"Best FastSurvivalSVM parameters:\", svm_grid.best_params_)\n",
    "    risk_scores_svm_train = best_svm.predict(X_train)\n",
    "    risk_scores_svm_test = best_svm.predict(X_test)\n",
    "\n",
    "    # Cross-validation C-index\n",
    "    print(\"\\n=== Cross-Validation Results ===\")\n",
    "    print(\"\\nComputing average C-index for each base model...\")\n",
    "    cv_results = compute_cv_cindices(X_train, y_train, X_test, y_test, cv=5)\n",
    "    print(\"\\nTest Set - Average C-index from cross-validation:\")\n",
    "    print(\"  RSF:             {:.4f}\".format(cv_results[\"RSF\"]))\n",
    "    print(\"  GBSA:            {:.4f}\".format(cv_results[\"GBSA\"]))\n",
    "    print(\"  FastSurvivalSVM: {:.4f}\".format(cv_results[\"FastSurvivalSVM\"]))\n",
    "\n",
    "    # Training set C-index\n",
    "    print(\"\\nComputing average C-index for training set...\")\n",
    "    train_cv_results = compute_cv_cindices(X_train, y_train, X_train, y_train, cv=5)\n",
    "    print(\"\\nTraining Set - Average C-index from cross-validation:\")\n",
    "    print(\"  RSF:             {:.4f}\".format(train_cv_results[\"RSF\"]))\n",
    "    print(\"  GBSA:            {:.4f}\".format(train_cv_results[\"GBSA\"]))\n",
    "    print(\"  FastSurvivalSVM: {:.4f}\".format(train_cv_results[\"FastSurvivalSVM\"]))\n",
    "\n",
    "    # Brier score calculation\n",
    "    print(\"\\n=== Brier Score Evaluation ===\")\n",
    "    survival_funcs = best_rsf.predict_survival_function(X_test)\n",
    "    max_time = max([fn.x[-1] for fn in survival_funcs])\n",
    "    time_points_brier = time_points[time_points <= max_time]\n",
    "    risk_preds = [fn(time_points_brier) for fn in survival_funcs]\n",
    "    brier_scores = custom_brier_score(y_test, risk_preds, time_points_brier)\n",
    "    print(\"\\nBrier scores (first 5 time points):\")\n",
    "    for t, score in zip(time_points_brier[:5], brier_scores[:5]):\n",
    "        print(f\"  Time {t:.2f}: Brier score = {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519aaaa5-bd5f-4e81-b0bf-15b6950466e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature selection and model selection--------------------------------clinical model selection\n",
    "\"\"\"\n",
    "Survival Analysis Pipeline with Unified Cross-Validation Folds\n",
    "Current Date and Time (UTC): 2025-04-18 13:54:06\n",
    "Current User's Login: ZEHAILU-1\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "from sksurv.metrics import concordance_index_censored, NoComparablePairException\n",
    "\n",
    "# Global variable definitions\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "def print_session_info():\n",
    "    \"\"\"Print session information, including date/time and user login info\"\"\"\n",
    "    current_time = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    current_user = os.getenv('USERNAME', 'Unknown')\n",
    "    \n",
    "    print(\"=== Session Information ===\")\n",
    "    print(f\"Date and Time (UTC): {current_time}\")\n",
    "    print(f\"User Login: {current_user}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "def compute_baseline_survival(y_train):\n",
    "    \"\"\"Compute the Kaplanâ€“Meier baseline survival function\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": y_train[\"lenfol\"],\n",
    "        \"event\": y_train[\"fstat\"]\n",
    "    })\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[\"time\"], event_observed=df[\"event\"])\n",
    "    \n",
    "    def baseline_surv(t):\n",
    "        return np.interp(t, kmf.survival_function_.index.values,\n",
    "                        kmf.survival_function_[\"KM_estimate\"].values,\n",
    "                        left=1.0, right=kmf.survival_function_[\"KM_estimate\"].values[-1])\n",
    "    return baseline_surv\n",
    "\n",
    "def preprocess_data(data_x, data_y):\n",
    "    \"\"\"Preprocess data with feature selection steps\"\"\"\n",
    "    # print(\"\\n=== Feature Selection Steps ===\")\n",
    "    # print(f\"1. Original number of features: {len(data_x.columns)}\")\n",
    "    # print(\"Original features:\")\n",
    "    for feat in data_x.columns:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    from sksurv.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    x_encoded = encoder.fit_transform(data_x)\n",
    "    try:\n",
    "        feature_names = encoder.get_feature_names_out(data_x.columns)\n",
    "    except AttributeError:\n",
    "        feature_names = np.array(data_x.columns)\n",
    "    \n",
    "    print(f\"\\n2. After one-hot encoding: {x_encoded.shape[1]} features\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(x_encoded)\n",
    "    \n",
    "    vt = VarianceThreshold(threshold=0.1)\n",
    "    X_scaled = vt.fit_transform(X_scaled)\n",
    "    feature_names = feature_names[vt.get_support()]\n",
    "    print(f\"\\n3. After variance thresholding: {len(feature_names)} features\")\n",
    "    print(\"Features retained after variance thresholding:\")\n",
    "    for feat in feature_names:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "    cox_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alphas=[0.1])\n",
    "    try:\n",
    "        cox_model.fit(X_scaled, data_y)\n",
    "    except Exception as e:\n",
    "        print(f\"Cox model fitting error: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    cox_coef = pd.Series(cox_model.coef_[:, 0], index=feature_names)\n",
    "    selected_features_cox = cox_coef[cox_coef != 0].index\n",
    "    print(f\"\\n4. After Cox regression: {len(selected_features_cox)} features\")\n",
    "    print(\"Features selected by Cox regression:\")\n",
    "    for feat in selected_features_cox:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    elastic_net = ElasticNetCV(cv=N_SPLITS, random_state=RANDOM_STATE)\n",
    "    X_train_selected_cox = X_scaled[:, np.isin(feature_names, selected_features_cox)]\n",
    "    elastic_net.fit(X_train_selected_cox, data_y[\"lenfol\"])\n",
    "    coef_abs = np.abs(elastic_net.coef_)\n",
    "    top_10_indices = np.argsort(coef_abs)[-10:]\n",
    "    selected_features_elasticnet = selected_features_cox[top_10_indices]\n",
    "    \n",
    "    print(f\"\\n5. Final features after ElasticNetCV: {len(selected_features_elasticnet)} features\")\n",
    "    print(\"Final selected features (ranked by importance):\")\n",
    "    for i, feat in enumerate(selected_features_elasticnet, 1):\n",
    "        coef = elastic_net.coef_[top_10_indices[i-1]]\n",
    "        print(f\"   {i}. {feat} (coefficient: {coef:.4f})\")\n",
    "    \n",
    "    final_feature_indices = np.isin(feature_names, selected_features_elasticnet)\n",
    "    X_final = X_scaled[:, final_feature_indices]\n",
    "    selected_features = feature_names[final_feature_indices]\n",
    "    \n",
    "    # Save feature selection results\n",
    "    feature_selection_results = pd.DataFrame({\n",
    "        'Feature': selected_features_elasticnet,\n",
    "        'Coefficient': elastic_net.coef_[top_10_indices]\n",
    "    })\n",
    "    feature_selection_results = feature_selection_results.sort_values('Coefficient', ascending=False)\n",
    "    output_file = 'feature_selection_results.xlsx'\n",
    "    feature_selection_results.to_excel(output_file, index=False)\n",
    "    print(f\"\\nFeature selection results have been saved to: {output_file}\")\n",
    "    \n",
    "    return X_final, selected_features, encoder, scaler, vt\n",
    "\n",
    "def compute_cv_cindices(X_train, y_train, X_test, y_test, cv=N_SPLITS):\n",
    "    \"\"\"Compute cross-validated concordance indices\"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "    }\n",
    "    \n",
    "    cv_scores = {\n",
    "        \"FastSurvivalSVM\": []\n",
    "    }\n",
    "    \n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # FastSurvivalSVM\n",
    "        svm_grid = GridSearchCV(\n",
    "            FastSurvivalSVM(random_state=RANDOM_STATE),\n",
    "            param_grid=param_grid,\n",
    "            cv=3,\n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        svm_grid.fit(X_tr, y_tr)\n",
    "        best_svm = svm_grid.best_estimator_\n",
    "        pred_svm = best_svm.predict(X_test)\n",
    "        try:\n",
    "            cindex_svm = concordance_index_censored(y_test[\"fstat\"], y_test[\"lenfol\"], pred_svm)[0]\n",
    "        except NoComparablePairException:\n",
    "            cindex_svm = np.nan\n",
    "        cv_scores[\"FastSurvivalSVM\"].append(cindex_svm)\n",
    "        \n",
    "        print(f\"Fold {fold} complete.\")\n",
    "        fold += 1\n",
    "\n",
    "    # Remove NaNs from scores\n",
    "    cv_scores[\"FastSurvivalSVM\"] = [score for score in cv_scores[\"FastSurvivalSVM\"] if not np.isnan(score)]\n",
    "    if len(cv_scores[\"FastSurvivalSVM\"]) == 0:\n",
    "        raise ValueError(\"No valid C-index scores calculated.\")\n",
    "    \n",
    "    return {method: np.mean(scores) for method, scores in cv_scores.items()}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print_session_info()\n",
    "    \n",
    "    # File paths\n",
    "    file_mod1 = \"xxxxxxxxxxxxxxxxxxx\"\n",
    "    file_mod2 = \"xxxxxxxxxxxxxxxxxxx\"\n",
    "    \n",
    "    # Process Modality 1\n",
    "    print(\"\\nProcessing Modality 1 data...\")\n",
    "    data_mod1 = pd.read_excel(file_mod1)\n",
    "    data_x1 = data_mod1.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    data_y = np.array([(bool(status), float(time))\n",
    "                       for status, time in zip(data_mod1[\"event\"], data_mod1[\"time\"])],\n",
    "                      dtype=[(\"fstat\", \"?\"), (\"lenfol\", \"f8\")])\n",
    "    \n",
    "    X1_processed, selected_features_mod1, encoder1, scaler1, vt1 = preprocess_data(data_x1, data_y)\n",
    "    \n",
    "    # Process Modality 2\n",
    "    print(\"\\nProcessing Modality 2 data...\")\n",
    "    data_mod2 = pd.read_excel(file_mod2)\n",
    "    data_x2 = data_mod2.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    X2_processed, selected_features_mod2, encoder2, scaler2, vt2 = preprocess_data(data_x2, data_y)\n",
    "    \n",
    "    # Train-test split for both modalities\n",
    "    X_train1, X_test1, y_train, y_test = train_test_split(\n",
    "        X1_processed, data_y,\n",
    "        test_size=0.3, random_state=RANDOM_STATE, stratify=data_y[\"fstat\"]\n",
    "    )\n",
    "    X_train2, X_test2, _, _ = train_test_split(\n",
    "        X2_processed, data_y,\n",
    "        test_size=0.3, random_state=RANDOM_STATE, stratify=data_y[\"fstat\"]\n",
    "    )\n",
    "    \n",
    "    # Cross-validation for both modalities\n",
    "    print(\"\\n=== Cross-Validation Results ===\")\n",
    "    \n",
    "    print(\"\\nModality 1 (FastSurvivalSVM):\")\n",
    "    cv_results1 = compute_cv_cindices(X_train1, y_train, X_test1, y_test)\n",
    "    print(\"Average C-index: {:.4f}\".format(cv_results1[\"FastSurvivalSVM\"]))\n",
    "    \n",
    "    print(\"\\nModality 2 (FastSurvivalSVM):\")\n",
    "    cv_results2 = compute_cv_cindices(X_train2, y_train, X_test2, y_test)\n",
    "    print(\"Average C-index: {:.4f}\".format(cv_results2[\"FastSurvivalSVM\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5df6076-1799-416f-bdd4-0364f77edcc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# two model survival model infusion\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "from sksurv.metrics import concordance_index_censored, NoComparablePairException\n",
    "\n",
    "# Global variables for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "def preprocess_data(data_x, data_y):\n",
    "    # Print original features\n",
    "    for feat in data_x.columns:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    from sksurv.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    x_encoded = encoder.fit_transform(data_x)\n",
    "    try:\n",
    "        feature_names = encoder.get_feature_names_out(data_x.columns)\n",
    "    except AttributeError:\n",
    "        feature_names = np.array(data_x.columns)\n",
    "    \n",
    "    print(f\"\\nAfter one-hot encoding: {x_encoded.shape[1]} features\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(x_encoded)\n",
    "    \n",
    "    vt = VarianceThreshold(threshold=0.1)\n",
    "    X_scaled_vt = vt.fit_transform(X_scaled)\n",
    "    feature_names_vt = feature_names[vt.get_support()]\n",
    "    print(f\"\\nAfter variance thresholding: {len(feature_names_vt)} features\")\n",
    "    for feat in feature_names_vt:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "    cox_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alphas=[0.1])\n",
    "    try:\n",
    "        cox_model.fit(X_scaled_vt, data_y)\n",
    "    except Exception as e:\n",
    "        print(f\"Cox model fitting error: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "    cox_coef = pd.Series(cox_model.coef_[:, 0], index=feature_names_vt)\n",
    "    selected_features_cox = cox_coef[cox_coef != 0].index\n",
    "    print(f\"\\nAfter Cox regression: {len(selected_features_cox)} features\")\n",
    "    for feat in selected_features_cox:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    elastic_net = ElasticNetCV(cv=N_SPLITS, random_state=RANDOM_STATE)\n",
    "    X_train_selected_cox = X_scaled_vt[:, np.isin(feature_names_vt, selected_features_cox)]\n",
    "    elastic_net.fit(X_train_selected_cox, data_y[\"lenfol\"])\n",
    "    coef_abs = np.abs(elastic_net.coef_)\n",
    "    top_10_indices = np.argsort(coef_abs)[-10:]\n",
    "    selected_features_elasticnet = selected_features_cox[top_10_indices]\n",
    "    \n",
    "    print(f\"\\nFinal features after ElasticNetCV: {len(selected_features_elasticnet)} features\")\n",
    "    for i, feat in enumerate(selected_features_elasticnet, 1):\n",
    "        coef = elastic_net.coef_[top_10_indices[i-1]]\n",
    "        print(f\"   {i}. {feat} (coefficient: {coef:.4f})\")\n",
    "    \n",
    "    # Save feature selection results to Excel if needed\n",
    "    feature_selection_results = pd.DataFrame({\n",
    "        \"Feature\": selected_features_elasticnet,\n",
    "        \"Coefficient\": elastic_net.coef_[top_10_indices]\n",
    "    }).sort_values(\"Coefficient\", ascending=False)\n",
    "    feature_selection_results.to_excel(\"feature_selection_results.xlsx\", index=False)\n",
    "    print(\"\\nFeature selection results have been saved to: feature_selection_results.xlsx\")\n",
    "    \n",
    "    final_feature_indices = np.isin(feature_names_vt, selected_features_elasticnet)\n",
    "    X_final = X_scaled_vt[:, final_feature_indices]\n",
    "    selected_features = selected_features_elasticnet\n",
    "    \n",
    "    return X_final, selected_features, encoder, scaler, vt, feature_names_vt\n",
    "\n",
    "def compute_cv_cindices(X_train, y_train, X_test, y_test, cv=N_SPLITS):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    param_grid = {\"alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "    cv_scores = {\"FastSurvivalSVM\": []}\n",
    "    \n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "        svm_grid = GridSearchCV(\n",
    "            FastSurvivalSVM(random_state=RANDOM_STATE),\n",
    "            param_grid=param_grid,\n",
    "            cv=3,\n",
    "            scoring=\"neg_mean_squared_error\"\n",
    "        )\n",
    "        svm_grid.fit(X_tr, y_tr)\n",
    "        best_svm = svm_grid.best_estimator_\n",
    "        pred_svm = best_svm.predict(X_test)\n",
    "        try:\n",
    "            cindex_svm = concordance_index_censored(y_test[\"fstat\"], y_test[\"lenfol\"], pred_svm)[0]\n",
    "        except NoComparablePairException:\n",
    "            cindex_svm = np.nan\n",
    "        cv_scores[\"FastSurvivalSVM\"].append(cindex_svm)\n",
    "        print(f\"Fold {fold} complete.\")\n",
    "        fold += 1\n",
    "\n",
    "    cv_scores[\"FastSurvivalSVM\"] = [score for score in cv_scores[\"FastSurvivalSVM\"] if not np.isnan(score)]\n",
    "    if not cv_scores[\"FastSurvivalSVM\"]:\n",
    "        raise ValueError(\"No valid C-index scores calculated.\")\n",
    "    return {\"FastSurvivalSVM\": np.mean(cv_scores[\"FastSurvivalSVM\"])}\n",
    "\n",
    "def compute_cv_cindices_bma(X1, X2, y, model1, model2, cv=N_SPLITS):\n",
    "    tau = 0.1  # control sensitivity parameter\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores_train = []\n",
    "    cv_scores_val = []\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(X1):\n",
    "        X1_tr, X1_val = X1[train_idx], X1[val_idx]\n",
    "        X2_tr, X2_val = X2[train_idx], X2[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model1.fit(X1_tr, y_tr)\n",
    "        model2.fit(X2_tr, y_tr)\n",
    "        pred1_train = model1.predict(X1_tr)\n",
    "        pred2_train = model2.predict(X2_tr)\n",
    "        try:\n",
    "            cindex_tr1 = concordance_index_censored(y_tr[\"fstat\"], y_tr[\"lenfol\"], pred1_train)[0]\n",
    "        except NoComparablePairException:\n",
    "            cindex_tr1 = 0.5\n",
    "        try:\n",
    "            cindex_tr2 = concordance_index_censored(y_tr[\"fstat\"], y_tr[\"lenfol\"], pred2_train)[0]\n",
    "        except NoComparablePairException:\n",
    "            cindex_tr2 = 0.5\n",
    "        \n",
    "        weight1 = np.exp((cindex_tr1 - 0.5) / tau)\n",
    "        weight2 = np.exp((cindex_tr2 - 0.5) / tau)\n",
    "        total_weight = weight1 + weight2\n",
    "        normalized_weight1 = weight1 / total_weight\n",
    "        normalized_weight2 = weight2 / total_weight\n",
    "        \n",
    "        combined_train = normalized_weight1 * pred1_train + normalized_weight2 * pred2_train\n",
    "        try:\n",
    "            cindex_combined_train = concordance_index_censored(\n",
    "                y_tr[\"fstat\"], y_tr[\"lenfol\"], combined_train)[0]\n",
    "        except NoComparablePairException:\n",
    "            cindex_combined_train = np.nan\n",
    "        \n",
    "        pred1_val = model1.predict(X1_val)\n",
    "        pred2_val = model2.predict(X2_val)\n",
    "        combined_val = normalized_weight1 * pred1_val + normalized_weight2 * pred2_val\n",
    "        try:\n",
    "            cindex_val = concordance_index_censored(\n",
    "                y_val[\"fstat\"], y_val[\"lenfol\"], combined_val)[0]\n",
    "        except NoComparablePairException:\n",
    "            cindex_val = np.nan\n",
    "        \n",
    "        cv_scores_train.append(cindex_combined_train)\n",
    "        cv_scores_val.append(cindex_val)\n",
    "        \n",
    "        print(f\"BMA Fusion - Fold {fold} complete (weights: {normalized_weight1:.3f}, {normalized_weight2:.3f})\")\n",
    "        fold += 1\n",
    "    \n",
    "    cv_scores_train = [score for score in cv_scores_train if not np.isnan(score)]\n",
    "    cv_scores_val = [score for score in cv_scores_val if not np.isnan(score)]\n",
    "    if not cv_scores_train or not cv_scores_val:\n",
    "        raise ValueError(\"No valid C-index scores calculated for BMA fusion.\")\n",
    "    return np.mean(cv_scores_train), np.mean(cv_scores_val)\n",
    "\n",
    "def main():\n",
    "    print_session_info()\n",
    "    \n",
    "    file_mod1 = \"xxxxxxxxxxxxxxxxxxx\"\n",
    "    file_mod2 = \"xxxxxxxxxxxxxxxxxxx\"\n",
    "    \n",
    "    # Process Modality 1\n",
    "    print(\"\\nProcessing Modality 1 data...\")\n",
    "    data_mod1 = pd.read_excel(file_mod1)\n",
    "    data_x1 = data_mod1.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    data_y = np.array([(bool(status), float(time))\n",
    "                       for status, time in zip(data_mod1[\"event\"], data_mod1[\"time\"])],\n",
    "                      dtype=[(\"fstat\", \"?\"), (\"lenfol\", \"f8\")])\n",
    "    X1_processed, selected_features_mod1, encoder1, scaler1, vt1, feature_names_mod1 = preprocess_data(data_x1, data_y)\n",
    "    \n",
    "    # Process Modality 2\n",
    "    print(\"\\nProcessing Modality 2 data...\")\n",
    "    data_mod2 = pd.read_excel(file_mod2)\n",
    "    data_x2 = data_mod2.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    X2_processed, selected_features_mod2, encoder2, scaler2, vt2, feature_names_mod2 = preprocess_data(data_x2, data_y)\n",
    "    \n",
    "    X_train1, X_test1, y_train, y_test = train_test_split(\n",
    "        X1_processed, data_y,\n",
    "        test_size=0.3, random_state=RANDOM_STATE, stratify=data_y[\"fstat\"]\n",
    "    )\n",
    "    X_train2, X_test2, _, _ = train_test_split(\n",
    "        X2_processed, data_y,\n",
    "        test_size=0.3, random_state=RANDOM_STATE, stratify=data_y[\"fstat\"]\n",
    "    )\n",
    "    \n",
    "    # Train individual FastSurvivalSVM models\n",
    "    print(\"\\n=== Training Individual Models ===\")\n",
    "    svm_param_grid = {\"alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "    \n",
    "    print(\"\\nTuning Modality 1 (FastSurvivalSVM)...\")\n",
    "    grid_svm1 = GridSearchCV(\n",
    "        FastSurvivalSVM(random_state=RANDOM_STATE),\n",
    "        param_grid=svm_param_grid,\n",
    "        cv=N_SPLITS,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "    grid_svm1.fit(X_train1, y_train)\n",
    "    best_svm_model1 = grid_svm1.best_estimator_\n",
    "    print(\"Modality 1 best parameters:\", grid_svm1.best_params_)\n",
    "    \n",
    "    print(\"\\nTuning Modality 2 (FastSurvivalSVM)...\")\n",
    "    grid_svm2 = GridSearchCV(\n",
    "        FastSurvivalSVM(random_state=RANDOM_STATE),\n",
    "        param_grid=svm_param_grid,\n",
    "        cv=N_SPLITS,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "    grid_svm2.fit(X_train2, y_train)\n",
    "    best_svm_model2 = grid_svm2.best_estimator_\n",
    "    print(\"Modality 2 best parameters:\", grid_svm2.best_params_)\n",
    "    \n",
    "    print(\"\\n=== Cross-Validation Results for Individual Models ===\")\n",
    "    print(\"Modality 1 (FastSurvivalSVM):\")\n",
    "    cv_results1 = compute_cv_cindices(X_train1, y_train, X_test1, y_test)\n",
    "    print(\"Average C-index: {:.4f}\".format(cv_results1[\"FastSurvivalSVM\"]))\n",
    "    \n",
    "    print(\"\\nModality 2 (FastSurvivalSVM):\")\n",
    "    cv_results2 = compute_cv_cindices(X_train2, y_train, X_test2, y_test)\n",
    "    print(\"Average C-index: {:.4f}\".format(cv_results2[\"FastSurvivalSVM\"]))\n",
    "    \n",
    "    print(\"\\n=== Bayesian Model Averaging Fusion Results ===\")\n",
    "    train_cindex_bma, val_cindex_bma = compute_cv_cindices_bma(\n",
    "        X_train1, X_train2, y_train,\n",
    "        best_svm_model1, best_svm_model2\n",
    "    )\n",
    "    print(f\"Training Set C-index: {train_cindex_bma:.4f}\")\n",
    "    print(f\"Validation Set C-index: {val_cindex_bma:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"Model\": [\"Modality 1\", \"Modality 2\", \"BMA Fusion\"],\n",
    "        \"Training C-index\": [cv_results1[\"FastSurvivalSVM\"], cv_results2[\"FastSurvivalSVM\"], train_cindex_bma],\n",
    "        \"Validation C-index\": [cv_results1[\"FastSurvivalSVM\"], cv_results2[\"FastSurvivalSVM\"], val_cindex_bma]\n",
    "    })\n",
    "    results_df.to_excel(\"model_comparison_results.xlsx\", index=False)\n",
    "    print(\"\\nResults have been saved to 'model_comparison_results.xlsx'\")\n",
    "    \n",
    "    # Save preprocessing objects and trained models to disk for external validation.\n",
    "    with open(\"modality1_preprocessing.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"encoder\": encoder1,\n",
    "            \"scaler\": scaler1,\n",
    "            \"vt\": vt1,\n",
    "            \"feature_names\": feature_names_mod1,\n",
    "            \"selected_features\": selected_features_mod1\n",
    "        }, f)\n",
    "    with open(\"modality2_preprocessing.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"encoder\": encoder2,\n",
    "            \"scaler\": scaler2,\n",
    "            \"vt\": vt2,\n",
    "            \"feature_names\": feature_names_mod2,\n",
    "            \"selected_features\": selected_features_mod2\n",
    "        }, f)\n",
    "    with open(\"best_svm_models.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"modality1\": best_svm_model1,\n",
    "            \"modality2\": best_svm_model2\n",
    "        }, f)\n",
    "    print(\"\\nPreprocessing objects and trained models have been saved to disk.\")\n",
    "    \n",
    "    # Return variables for external use.\n",
    "    return (X_train1, X_train2, y_train, X_test1, X_test2, y_test, \n",
    "        best_svm_model1, best_svm_model2, selected_features_mod1, selected_features_mod2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (X_train1, X_train2, y_train, X_test1, X_test2, y_test, best_svm_model1, best_svm_model2, selected_features_mod1, selected_features_mod2) = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204b4013-300b-4e74-8524-bea7f4140328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature selection visualization\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "from sksurv.metrics import concordance_index_censored, NoComparablePairException\n",
    "\n",
    "# Global variables for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "def visualize_elastic_net_selection(feature_names, coefficients, save_path=None, title=\"Elastic Net Feature Coefficients\"):\n",
    "    \"\"\"\n",
    "    Visualize the features selected by Elastic Net regression and their coefficients, and save the plot as a PDF.\n",
    "    \"\"\"\n",
    "    feature_names = np.array(feature_names)\n",
    "    coefficients = np.array(coefficients)\n",
    "    sorted_idx = np.argsort(np.abs(coefficients))[::-1]\n",
    "    feature_names_sorted = feature_names[sorted_idx]\n",
    "    coefficients_sorted = coefficients[sorted_idx]\n",
    "\n",
    "    colors = ['#e74c3c' if coef < 0 else '#3498db' for coef in coefficients_sorted]\n",
    "\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    ax = plt.gca()\n",
    "    bars = ax.barh(range(len(coefficients_sorted)), coefficients_sorted, color=colors)\n",
    "    ax.set_yticks(range(len(coefficients_sorted)))\n",
    "    ax.set_yticklabels(feature_names_sorted, fontsize=32, fontname='Arial')\n",
    "    # ax.set_xlabel('Coefficient Value', fontsize=32, fontname='Arial')\n",
    "    ax.set_title(title, fontsize=32, fontname='Arial')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xticks(fontsize=28, fontname='Arial')\n",
    "    # Place ticks and labels on the right side\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.set_ylabel(' ', fontsize=32, fontname='Arial', labelpad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def preprocess_data(data_x, data_y, vis_path=None, vis_title=None):\n",
    "    # Print original features\n",
    "    for feat in data_x.columns:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    from sksurv.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder()\n",
    "    x_encoded = encoder.fit_transform(data_x)\n",
    "    try:\n",
    "        feature_names = encoder.get_feature_names_out(data_x.columns)\n",
    "    except AttributeError:\n",
    "        feature_names = np.array(data_x.columns)\n",
    "    \n",
    "    print(f\"\\nAfter one-hot encoding: {x_encoded.shape[1]} features\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(x_encoded)\n",
    "    \n",
    "    vt = VarianceThreshold(threshold=0.1)\n",
    "    X_scaled_vt = vt.fit_transform(X_scaled)\n",
    "    feature_names_vt = feature_names[vt.get_support()]\n",
    "    print(f\"\\nAfter variance thresholding: {len(feature_names_vt)} features\")\n",
    "    for feat in feature_names_vt:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "    cox_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alphas=[0.1])\n",
    "    try:\n",
    "        cox_model.fit(X_scaled_vt, data_y)\n",
    "    except Exception as e:\n",
    "        print(f\"Cox model fitting error: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "    cox_coef = pd.Series(cox_model.coef_[:, 0], index=feature_names_vt)\n",
    "    selected_features_cox = cox_coef[cox_coef != 0].index\n",
    "    print(f\"\\nAfter Cox regression: {len(selected_features_cox)} features\")\n",
    "    for feat in selected_features_cox:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    elastic_net = ElasticNetCV(cv=N_SPLITS, random_state=RANDOM_STATE)\n",
    "    X_train_selected_cox = X_scaled_vt[:, np.isin(feature_names_vt, selected_features_cox)]\n",
    "    elastic_net.fit(X_train_selected_cox, data_y[\"lenfol\"])\n",
    "    coef_abs = np.abs(elastic_net.coef_)\n",
    "    top_10_indices = np.argsort(coef_abs)[-10:]\n",
    "    selected_features_elasticnet = selected_features_cox[top_10_indices]\n",
    "    \n",
    "    print(f\"\\nFinal features after ElasticNetCV: {len(selected_features_elasticnet)} features\")\n",
    "    for i, feat in enumerate(selected_features_elasticnet, 1):\n",
    "        coef = elastic_net.coef_[top_10_indices[i-1]]\n",
    "        print(f\"   {i}. {feat} (coefficient: {coef:.4f})\")\n",
    "\n",
    "    # Visualize Elastic Net selected features\n",
    "    if vis_path is not None:\n",
    "        visualize_elastic_net_selection(\n",
    "            selected_features_elasticnet,\n",
    "            elastic_net.coef_[top_10_indices],\n",
    "            save_path=vis_path,\n",
    "            title=vis_title if vis_title else \"Elastic Net Feature Coefficients\"\n",
    "        )\n",
    "\n",
    "    # Save feature selection results to Excel if needed\n",
    "    feature_selection_results = pd.DataFrame({\n",
    "        \"Feature\": selected_features_elasticnet,\n",
    "        \"Coefficient\": elastic_net.coef_[top_10_indices]\n",
    "    }).sort_values(\"Coefficient\", ascending=False)\n",
    "    feature_selection_results.to_excel(\"feature_selection_results.xlsx\", index=False)\n",
    "    print(\"\\nFeature selection results have been saved to: feature_selection_results.xlsx\")\n",
    "    \n",
    "    final_feature_indices = np.isin(feature_names_vt, selected_features_elasticnet)\n",
    "    X_final = X_scaled_vt[:, final_feature_indices]\n",
    "    selected_features = selected_features_elasticnet\n",
    "    \n",
    "    return X_final, selected_features, encoder, scaler, vt, feature_names_vt\n",
    "\n",
    "def print_session_info():\n",
    "    \"\"\"Print session information, including date/time and user login info\"\"\"\n",
    "    current_time = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    current_user = os.getenv('USERNAME', 'Unknown')\n",
    "    \n",
    "    print(\"=== Session Information ===\")\n",
    "    print(f\"Date and Time (UTC): {current_time}\")\n",
    "    print(f\"User Login: {current_user}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "def main():\n",
    "    print_session_info()\n",
    "    file_mod1 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "    file_mod2 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "    \n",
    "    # Process Modality 1\n",
    "    print(\"\\nProcessing Modality 1 data...\")\n",
    "    data_mod1 = pd.read_excel(file_mod1)\n",
    "    data_x1 = data_mod1.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    data_y = np.array([(bool(status), float(time))\n",
    "                       for status, time in zip(data_mod1[\"event\"], data_mod1[\"time\"])],\n",
    "                      dtype=[(\"fstat\", \"?\"), (\"lenfol\", \"f8\")])\n",
    "    X1_processed, selected_features_mod1, encoder1, scaler1, vt1, feature_names_mod1 = preprocess_data(\n",
    "        data_x1, data_y, vis_path=\"elasticnet_selection_mod1.pdf\", vis_title=\"Radiomics Feature\")\n",
    "    \n",
    "    # Process Modality 2\n",
    "    print(\"\\nProcessing Modality 2 data...\")\n",
    "    data_mod2 = pd.read_excel(file_mod2)\n",
    "    data_x2 = data_mod2.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "    X2_processed, selected_features_mod2, encoder2, scaler2, vt2, feature_names_mod2 = preprocess_data(\n",
    "        data_x2, data_y, vis_path=\"elasticnet_selection_mod2.pdf\", vis_title=\"Clinical Feature\")\n",
    "    \n",
    "    # Further code as needed, following your main workflow\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd4cf88-edce-4f95-b9b8-91f26d37024d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature distribution visualization\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def plot_risk_stratified_heatmap(\n",
    "    X1, X2, selected_features_mod1, selected_features_mod2, risk_scores,\n",
    "    n_low=None, n_high=None, abbr_prefix=\"RF\"\n",
    "):\n",
    "    \"\"\"\n",
    "    X1: Radiomic feature data (samples Ã— radiomic features, need abbreviation)\n",
    "    X2: Clinical feature data (samples Ã— clinical features, keep original names)\n",
    "    selected_features_mod1: Radiomic feature names (to abbreviate)\n",
    "    selected_features_mod2: Clinical feature names (keep original names)\n",
    "    risk_scores: Risk scores\n",
    "    \"\"\"\n",
    "    # Set global font\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "    plt.rcParams[\"axes.labelsize\"] = 22\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 24\n",
    "    plt.rcParams[\"legend.fontsize\"] = 28\n",
    "\n",
    "    # Only abbreviate radiomic features (X1, selected_features_mod1), keep clinical features (X2, selected_features_mod2) as is\n",
    "    radiomic_features = list(selected_features_mod1)\n",
    "    clinical_features = list(selected_features_mod2)\n",
    "    radiomic_short = [f\"{abbr_prefix} {i+1}\" for i in range(len(radiomic_features))]\n",
    "    abbr_df = pd.DataFrame({\n",
    "        \"Radiomic Abbreviation\": radiomic_short,\n",
    "        \"Original Radiomic Feature Name\": radiomic_features\n",
    "    })\n",
    "    abbr_df.to_csv(\"risk_heatmap_radiomic_abbreviation.csv\", index=False)\n",
    "\n",
    "    # Construct full feature name list\n",
    "    full_feature_names = radiomic_short + clinical_features\n",
    "\n",
    "    # Construct dataframe\n",
    "    df1 = pd.DataFrame(X1, columns=radiomic_short)       # Radiomics (abbreviated)\n",
    "    df2 = pd.DataFrame(X2, columns=clinical_features)     # Clinical (original)\n",
    "    df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "    # Sort by risk score\n",
    "    sort_idx = np.argsort(risk_scores)\n",
    "    df_sorted = df.iloc[sort_idx]\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(df_sorted),\n",
    "        columns=df_sorted.columns\n",
    "    )\n",
    "\n",
    "    # Grouping\n",
    "    n_samples = len(df_scaled)\n",
    "    if n_low is None or n_high is None:\n",
    "        n_low = n_samples // 2\n",
    "        n_high = n_samples - n_low\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(22, 14))\n",
    "    gs = GridSpec(2, 1, height_ratios=[0.8, 10], hspace=0.02)\n",
    "\n",
    "    # Heatmap\n",
    "    ax_heat = fig.add_subplot(gs[1])\n",
    "    sns.heatmap(\n",
    "        df_scaled.T,\n",
    "        ax=ax_heat,\n",
    "        cmap='RdBu_r',\n",
    "        xticklabels=False,\n",
    "        yticklabels=True,\n",
    "        center=0,\n",
    "        vmin=-3,\n",
    "        vmax=3,\n",
    "        cbar_kws={'label': 'Standardized Value', 'pad': 0.01}\n",
    "    )\n",
    "    ax_heat.set_yticklabels(df_scaled.columns, fontsize=28, family='Arial')\n",
    "    # Set colorbar font size\n",
    "    cbar = ax_heat.collections[0].colorbar\n",
    "    cbar.set_label('Standardized Value', fontsize=30, family='Arial')\n",
    "\n",
    "    # Color band\n",
    "    ax_label = fig.add_subplot(gs[0])\n",
    "    ax_label.axvspan(0, n_low, facecolor='#D6EAF8', alpha=0.5)\n",
    "    ax_label.axvspan(n_low, n_samples, facecolor='#D5F5E3', alpha=0.5)\n",
    "    ax_label.text(n_low/2, 0.5, f'Low Risk Group (N={n_low})',\n",
    "                  ha='center', va='center', fontsize=30, fontweight='bold', family='Arial')\n",
    "    ax_label.text(n_low + n_high/2, 0.5, f'High Risk Group (N={n_high})',\n",
    "                  ha='center', va='center', fontsize=30, fontweight='bold', family='Arial')\n",
    "    ax_label.set_xlim(0, n_samples)\n",
    "    ax_label.set_ylim(0, 1)\n",
    "    ax_label.axis('off')\n",
    "    ax_label.set_position([ax_heat.get_position().x0,\n",
    "                           ax_label.get_position().y0,\n",
    "                           ax_heat.get_position().width,\n",
    "                           ax_label.get_position().height])\n",
    "\n",
    "    # Title (optional)\n",
    "    # plt.suptitle('Feature Distribution Heatmap Stratified by Risk Score',\n",
    "    #              fontsize=20, fontweight='bold', y=0.98, family='Arial')\n",
    "\n",
    "    # Risk score info (optional)\n",
    "    # plt.figtext(0.02, 0.02, f'Risk Score Median = {np.median(risk_scores):.3f}',\n",
    "    #             fontsize=15, family='Arial')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('risk_stratified_heatmap.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def print_session_info():\n",
    "    \"\"\"Print session information, including date/time and user login info\"\"\"\n",
    "    current_time = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    current_user = os.getenv('USERNAME', 'Unknown')\n",
    "    \n",
    "    print(\"=== Session Information ===\")\n",
    "    print(f\"Date and Time (UTC): {current_time}\")\n",
    "    print(f\"User Login: {current_user}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "def main():\n",
    "    print_session_info()\n",
    "    \n",
    "    # Load preprocessing objects\n",
    "    with open(\"modality1_preprocessing.pkl\", \"rb\") as f:\n",
    "        preprocessing_mod1 = pickle.load(f)\n",
    "        selected_features_mod1 = preprocessing_mod1[\"selected_features\"]\n",
    "    \n",
    "    with open(\"modality2_preprocessing.pkl\", \"rb\") as f:\n",
    "        preprocessing_mod2 = pickle.load(f)\n",
    "        selected_features_mod2 = preprocessing_mod2[\"selected_features\"]\n",
    "    \n",
    "    # Get risk scores\n",
    "    risk_scores1 = best_svm_model1.predict(X_train1)\n",
    "    risk_scores2 = best_svm_model2.predict(X_train2)\n",
    "    combined_risk_scores = (risk_scores1 + risk_scores2) / 2\n",
    "    \n",
    "    # Plot heatmap - corrected call\n",
    "    plot_risk_stratified_heatmap(\n",
    "        X_train1,  # Radiomic feature data\n",
    "        X_train2,  # Clinical feature data\n",
    "        list(selected_features_mod1),  # Radiomic feature names\n",
    "        list(selected_features_mod2),  # Clinical feature names\n",
    "        combined_risk_scores\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49220142-9aae-40d6-aa26-13108fc189d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# External Validation\n",
    "\"\"\"\n",
    "External Validation Code for Single-Modality and Fusion Models\n",
    "\n",
    "This code loads the preprocessing objects and trained models from the training stage (saved as pickle files)\n",
    "and then processes the external validation files.\n",
    "External files:\n",
    "    ext_file_mod1 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "    ext_file_mod2 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Load saved preprocessing objects and models\n",
    "with open(\"modality1_preprocessing.pkl\", \"rb\") as f:\n",
    "    mod1_preproc = pickle.load(f)\n",
    "with open(\"modality2_preprocessing.pkl\", \"rb\") as f:\n",
    "    mod2_preproc = pickle.load(f)\n",
    "with open(\"best_svm_models.pkl\", \"rb\") as f:\n",
    "    best_models = pickle.load(f)\n",
    "\n",
    "encoder1 = mod1_preproc[\"encoder\"]\n",
    "scaler1 = mod1_preproc[\"scaler\"]\n",
    "vt1 = mod1_preproc[\"vt\"]\n",
    "\n",
    "encoder2 = mod2_preproc[\"encoder\"]\n",
    "scaler2 = mod2_preproc[\"scaler\"]\n",
    "vt2 = mod2_preproc[\"vt\"]\n",
    "\n",
    "best_svm_model1 = best_models[\"modality1\"]\n",
    "best_svm_model2 = best_models[\"modality2\"]\n",
    "\n",
    "# External file paths\n",
    "ext_file_mod1 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "ext_file_mod2 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# Load External Data - Modality 1\n",
    "ext_mod1 = pd.read_excel(ext_file_mod1)\n",
    "data_x_ext1 = ext_mod1.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "data_y_ext = np.array(\n",
    "    [(bool(status), float(time))\n",
    "     for status, time in zip(ext_mod1[\"event\"], ext_mod1[\"time\"])],\n",
    "    dtype=[(\"fstat\", \"?\"), (\"lenfol\", \"f8\")]\n",
    ")\n",
    "\n",
    "# Load External Data - Modality 2\n",
    "ext_mod2 = pd.read_excel(ext_file_mod2)\n",
    "data_x_ext2 = ext_mod2.drop(columns=[\"time\", \"event\", \"label\"])\n",
    "# Assuming outcome info is the same, we reuse data_y_ext for modality 2.\n",
    "\n",
    "# Feature Extraction for External Data - Modality 1 (no feature selection)\n",
    "X_ext1_encoded = encoder1.transform(data_x_ext1)\n",
    "X_ext1_scaled = scaler1.transform(X_ext1_encoded)\n",
    "X_ext1_final = vt1.transform(X_ext1_scaled)\n",
    "\n",
    "# Feature Extraction for External Data - Modality 2 (no feature selection)\n",
    "X_ext2_encoded = encoder2.transform(data_x_ext2)\n",
    "X_ext2_scaled = scaler2.transform(X_ext2_encoded)\n",
    "X_ext2_final = vt2.transform(X_ext2_scaled)\n",
    "\n",
    "# External Validation with Individual Models\n",
    "risk_ext_mod1 = best_svm_model1.predict(X_ext1_final)\n",
    "risk_ext_mod2 = best_svm_model2.predict(X_ext2_final)\n",
    "\n",
    "try:\n",
    "    cindex_ext_mod1 = concordance_index_censored(\n",
    "        data_y_ext[\"fstat\"], data_y_ext[\"lenfol\"], risk_ext_mod1\n",
    "    )[0]\n",
    "except Exception:\n",
    "    cindex_ext_mod1 = np.nan\n",
    "\n",
    "try:\n",
    "    cindex_ext_mod2 = concordance_index_censored(\n",
    "        data_y_ext[\"fstat\"], data_y_ext[\"lenfol\"], risk_ext_mod2\n",
    "    )[0]\n",
    "except Exception:\n",
    "    cindex_ext_mod2 = np.nan\n",
    "\n",
    "# External Validation with Bayesian Model Averaging (BMA) Fusion\n",
    "tau = 0.1  # same as used during training\n",
    "weight1_ext = np.exp((cindex_ext_mod1 - 0.5) / tau) if not np.isnan(cindex_ext_mod1) else 1.0\n",
    "weight2_ext = np.exp((cindex_ext_mod2 - 0.5) / tau) if not np.isnan(cindex_ext_mod2) else 1.0\n",
    "total_weight_ext = weight1_ext + weight2_ext\n",
    "norm_weight1_ext = weight1_ext / total_weight_ext\n",
    "norm_weight2_ext = weight2_ext / total_weight_ext\n",
    "fusion_risk_ext = norm_weight1_ext * risk_ext_mod1 + norm_weight2_ext * risk_ext_mod2\n",
    "\n",
    "try:\n",
    "    cindex_ext_fusion = concordance_index_censored(\n",
    "        data_y_ext[\"fstat\"], data_y_ext[\"lenfol\"], fusion_risk_ext\n",
    "    )[0]\n",
    "except Exception:\n",
    "    cindex_ext_fusion = np.nan\n",
    "\n",
    "print(\"External Validation Results:\")\n",
    "print(\"Modality 1 C-index: {:.4f}\".format(cindex_ext_mod1))\n",
    "print(\"Modality 2 C-index: {:.4f}\".format(cindex_ext_mod2))\n",
    "print(\"Fusion Model C-index: {:.4f}\".format(cindex_ext_fusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e2996a4-f9f4-45b6-9ae0-577cd13ac966",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# survival risk stratification for overall cohort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def compute_fusion_risk(pred1, pred2, surv_data, tau=0.1):\n",
    "    cindex1 = concordance_index_censored(surv_data[\"fstat\"], surv_data[\"lenfol\"], pred1)[0]\n",
    "    cindex2 = concordance_index_censored(surv_data[\"fstat\"], surv_data[\"lenfol\"], pred2)[0]\n",
    "    weight1 = np.exp((cindex1 - 0.5) / tau)\n",
    "    weight2 = np.exp((cindex2 - 0.5) / tau)\n",
    "    total_weight = weight1 + weight2\n",
    "    norm_weight1 = weight1 / total_weight\n",
    "    norm_weight2 = weight2 / total_weight\n",
    "    fusion_risk = norm_weight1 * pred1 + norm_weight2 * pred2\n",
    "    return fusion_risk\n",
    "\n",
    "# Compute fusion risks for training, test, and external sets (assuming best_svm_model1, best_svm_model2,\n",
    "# X_train1, X_train2, y_train, X_test1, X_test2, y_test, X_ext1_final, X_ext2_final, data_y_ext are defined)\n",
    "tau = 0.1  # same parameter as before\n",
    "\n",
    "risk_train_mod1 = best_svm_model1.predict(X_train1)\n",
    "risk_train_mod2 = best_svm_model2.predict(X_train2)\n",
    "fusion_risk_train = compute_fusion_risk(risk_train_mod1, risk_train_mod2, y_train, tau=tau)\n",
    "\n",
    "risk_test_mod1 = best_svm_model1.predict(X_test1)\n",
    "risk_test_mod2 = best_svm_model2.predict(X_test2)\n",
    "fusion_risk_test = compute_fusion_risk(risk_test_mod1, risk_test_mod2, y_test, tau=tau)\n",
    "\n",
    "risk_ext_mod1 = best_svm_model1.predict(X_ext1_final)\n",
    "risk_ext_mod2 = best_svm_model2.predict(X_ext2_final)\n",
    "fusion_risk_ext = compute_fusion_risk(risk_ext_mod1, risk_ext_mod2, data_y_ext, tau=tau)\n",
    "\n",
    "median_cut = np.median(fusion_risk_train)\n",
    "print(\"Training set fusion risk median: {:.4f}\".format(median_cut))\n",
    "\n",
    "def assign_risk_group(risk, median_cut):\n",
    "    return np.where(risk >= median_cut, \"High\", \"Low\")\n",
    "\n",
    "group_train = assign_risk_group(fusion_risk_train, median_cut)\n",
    "group_test  = assign_risk_group(fusion_risk_test, median_cut)\n",
    "group_ext   = assign_risk_group(fusion_risk_ext, median_cut)\n",
    "\n",
    "def prepare_survival_df(surv_data, fusion_risk, risk_group):\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": surv_data[\"lenfol\"],\n",
    "        \"event\": surv_data[\"fstat\"],\n",
    "        \"fusion_risk\": fusion_risk,\n",
    "        \"risk_group\": risk_group\n",
    "    })\n",
    "    df[\"risk_flag\"] = (df[\"risk_group\"] == \"High\").astype(int)\n",
    "    return df\n",
    "\n",
    "df_train = prepare_survival_df(y_train, fusion_risk_train, group_train)\n",
    "df_test  = prepare_survival_df(y_test, fusion_risk_test, group_test)\n",
    "df_ext   = prepare_survival_df(data_y_ext, fusion_risk_ext, group_ext)\n",
    "\n",
    "def plot_km_and_cox(df, dataset_name=\"Dataset\", save_pdf=True):\n",
    "    # Color scheme remains the same\n",
    "    colors = {'High': '#FF6B6B', 'Low': '#4ECDC4'}\n",
    "\n",
    "    # Set global font to Arial\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "    # Convert time from days to months\n",
    "    df['time_months'] = df['time'] / 30.44\n",
    "\n",
    "    kmf_high = KaplanMeierFitter()\n",
    "    kmf_low = KaplanMeierFitter()\n",
    "\n",
    "    mask_high = df[\"risk_group\"] == \"High\"\n",
    "    mask_low = df[\"risk_group\"] == \"Low\"\n",
    "    T_high, E_high = df.loc[mask_high, \"time_months\"], df.loc[mask_high, \"event\"]\n",
    "    T_low, E_low = df.loc[mask_low, \"time_months\"], df.loc[mask_low, \"event\"]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), dpi=300)\n",
    "\n",
    "    # Plot KM curves\n",
    "    kmf_high.fit(T_high, event_observed=E_high, label=\"High Risk\")\n",
    "    kmf_low.fit(T_low, event_observed=E_low, label=\"Low Risk\")\n",
    "\n",
    "    ax = kmf_high.plot(ci_show=True,\n",
    "                      color=colors['High'],\n",
    "                      ci_alpha=0.2,\n",
    "                      linewidth=3,\n",
    "                      show_censors=False,\n",
    "                      ax=ax)\n",
    "\n",
    "    ax = kmf_low.plot(ax=ax,\n",
    "                     ci_show=True,\n",
    "                     color=colors['Low'],\n",
    "                     ci_alpha=0.2,\n",
    "                     linewidth=3,\n",
    "                     show_censors=False)\n",
    "\n",
    "    # Calculate number at risk\n",
    "    times = [0, 12, 24, 36]\n",
    "    high_risk_counts = []\n",
    "    low_risk_counts = []\n",
    "\n",
    "    for t in times:\n",
    "        high_risk_counts.append(sum(T_high >= t))\n",
    "        low_risk_counts.append(sum(T_low >= t))\n",
    "\n",
    "    # Cox regression and P-value\n",
    "    from lifelines.utils import ConvergenceWarning\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "    lr_result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(df[[\"time_months\", \"event\", \"risk_flag\"]], duration_col=\"time_months\", event_col=\"event\")\n",
    "    hr = cph.hazard_ratios_[\"risk_flag\"]\n",
    "    ci_lower = cph.summary.loc[\"risk_flag\", \"exp(coef) lower 95%\"]\n",
    "    ci_upper = cph.summary.loc[\"risk_flag\", \"exp(coef) upper 95%\"]\n",
    "    p_value = cph.summary.loc[\"risk_flag\", \"p\"]\n",
    "\n",
    "    if p_value < 0.001:\n",
    "        p_text = \"P < 0.001\"\n",
    "    else:\n",
    "        p_text = f\"P = {p_value:.3f}\"\n",
    "\n",
    "    # Combine HR and P-value display\n",
    "    textstr = f\"HR = {hr:.2f} (95% CI: {ci_lower:.2f}-{ci_upper:.2f})\\n{p_text}\"\n",
    "    plt.text(0.95, 0.95, textstr,\n",
    "            transform=ax.transAxes,\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(facecolor='none', edgecolor='none', pad=2.5),\n",
    "            fontsize=30, family=\"Arial\")\n",
    "\n",
    "    # Set title and labels\n",
    "    plt.title(f\"{dataset_name}\",\n",
    "              fontsize=30,\n",
    "              pad=20,\n",
    "              family=\"Arial\")\n",
    "    plt.xlabel(\"Months\", fontsize=30, family=\"Arial\")\n",
    "    plt.ylabel(\"Survival Probability\", fontsize=30, family=\"Arial\")\n",
    "\n",
    "    # Optimize legend\n",
    "    plt.legend(\n",
    "        loc='lower left',\n",
    "        frameon=True,\n",
    "        edgecolor='none',\n",
    "        shadow=False,\n",
    "        fontsize=30,\n",
    "        facecolor='none',\n",
    "        framealpha=0\n",
    "    )\n",
    "\n",
    "    # Beautify axis\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    # Set x-ticks\n",
    "    plt.xticks(times, fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "\n",
    "    # Adjust main plot position\n",
    "    plt.subplots_adjust(bottom=0.22)\n",
    "\n",
    "    # Remove x-axis tick labels\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    # Add time point labels\n",
    "    for i, t in enumerate(times):\n",
    "        plt.text(t, -0.08, str(t),\n",
    "                ha='center', va='bottom',\n",
    "                transform=ax.get_xaxis_transform(),\n",
    "                fontsize=30, family=\"Arial\")\n",
    "\n",
    "    # Calculate label position\n",
    "    x_min = min(times)\n",
    "    time_interval = times[1] - times[0]\n",
    "    label_x_pos = x_min - time_interval/4\n",
    "\n",
    "    # Risk group labels\n",
    "    plt.text(label_x_pos, -0.18, \"High risk\",\n",
    "            ha='right', va='center',\n",
    "            transform=ax.get_xaxis_transform(),\n",
    "            fontsize=30, family=\"Arial\")\n",
    "    plt.text(label_x_pos, -0.26, \"Low risk\",\n",
    "            ha='right', va='center',\n",
    "            transform=ax.get_xaxis_transform(),\n",
    "            fontsize=30, family=\"Arial\")\n",
    "\n",
    "    # Add numbers at risk\n",
    "    for i, (high_count, low_count) in enumerate(zip(high_risk_counts, low_risk_counts)):\n",
    "        # High risk data\n",
    "        plt.text(times[i], -0.18, str(high_count),\n",
    "                ha='center', va='center',\n",
    "                transform=ax.get_xaxis_transform(),\n",
    "                fontsize=30, family=\"Arial\")\n",
    "\n",
    "        # Low risk data\n",
    "        plt.text(times[i], -0.26, str(low_count),\n",
    "                ha='center', va='center',\n",
    "                transform=ax.get_xaxis_transform(),\n",
    "                fontsize=30, family=\"Arial\")\n",
    "\n",
    "    plt.xlim(times[0], times[-1])\n",
    "\n",
    "    # Save as PDF\n",
    "    if save_pdf:\n",
    "        pdfname = f\"KM_{dataset_name.replace(' ', '_')}.pdf\"\n",
    "        plt.savefig(pdfname, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {pdfname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{dataset_name} Log-rank Test: P-value = {lr_result.p_value:.4e}\")\n",
    "    print(f\"{dataset_name} Cox Regression: HR = {hr:.2f} (95% CI: {ci_lower:.2f}-{ci_upper:.2f}), {p_text}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Run code\n",
    "print(\"=== Training Set ===\")\n",
    "plot_km_and_cox(df_train, dataset_name=\"Training Set\")\n",
    "\n",
    "print(\"=== Test Set ===\")\n",
    "plot_km_and_cox(df_test, dataset_name=\"Test Set\")\n",
    "\n",
    "print(\"=== External Validation Set ===\")\n",
    "plot_km_and_cox(df_ext, dataset_name=\"External Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e1f8d5-bf4e-4b28-934e-6952e0c2e864",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# survival risk stratification for TACE/HAIC subgroup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load modality 2 survival data and external validation data\n",
    "file_mod2 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "data_mod2 = pd.read_excel(file_mod2)\n",
    "ext_file_mod2 = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "ext_mod2 = pd.read_excel(ext_file_mod2)\n",
    "\n",
    "# From modality 2 data, get T1H2TH0 information\n",
    "T1H2TH0_train = data_mod2['T1H2TH0'].iloc[:len(X_train1)].values\n",
    "T1H2TH0_test = data_mod2['T1H2TH0'].iloc[len(X_train1):].values\n",
    "T1H2TH0_ext = ext_mod2['T1H2TH0'].values\n",
    "\n",
    "# Update survival dataframe, adding T1H2TH0 information\n",
    "df_train['T1H2TH0'] = T1H2TH0_train\n",
    "df_test['T1H2TH0'] = T1H2TH0_test\n",
    "df_ext['T1H2TH0'] = T1H2TH0_ext\n",
    "\n",
    "# Get training set median fusion risk for cutoff calculation\n",
    "median_cut = np.median(fusion_risk_train)\n",
    "print(f\"Training set fusion risk median cut-off: {median_cut:.4f}\")\n",
    "\n",
    "def plot_km_by_TACE_HAIC(df, dataset_name=\"Dataset\", T1H2TH0_value=1, median_cut=None, save_pdf=True):\n",
    "    \"\"\"\n",
    "    Plot KM curves stratified by TACE+HAIC status, automatically skip empty groups, use Arial font, and save as PDF.\n",
    "    \"\"\"\n",
    "    TACE_HAIC_label = \"TACE+HAIC\" if T1H2TH0_value == 1 else \"TACE/HAIC\"\n",
    "    colors = {'High': '#FF6B6B', 'Low': '#4ECDC4'}\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "    # Filter specific group data\n",
    "    df_TACE_HAIC = df[df['T1H2TH0'] == T1H2TH0_value].copy()\n",
    "    df_TACE_HAIC['time_months'] = pd.to_numeric(df_TACE_HAIC['time'], errors='coerce') / 30.44\n",
    "    df_TACE_HAIC['risk_group'] = np.where(df_TACE_HAIC['fusion_risk'] >= median_cut, 'High', 'Low')\n",
    "    df_TACE_HAIC['risk_flag'] = (df_TACE_HAIC['risk_group'] == 'High').astype(int)\n",
    "\n",
    "    kmf_high = KaplanMeierFitter()\n",
    "    kmf_low = KaplanMeierFitter()\n",
    "\n",
    "    mask_high = df_TACE_HAIC[\"risk_group\"] == \"High\"\n",
    "    mask_low = df_TACE_HAIC[\"risk_group\"] == \"Low\"\n",
    "    T_high = pd.to_numeric(df_TACE_HAIC.loc[mask_high, \"time_months\"], errors='coerce').dropna()\n",
    "    E_high = pd.to_numeric(df_TACE_HAIC.loc[mask_high, \"event\"], errors='coerce').dropna()\n",
    "    T_low = pd.to_numeric(df_TACE_HAIC.loc[mask_low, \"time_months\"], errors='coerce').dropna()\n",
    "    E_low = pd.to_numeric(df_TACE_HAIC.loc[mask_low, \"event\"], errors='coerce').dropna()\n",
    "\n",
    "    # Handle empty groups\n",
    "    if len(T_high) == 0 or len(T_low) == 0:\n",
    "        print(f\"[{dataset_name}][{TACE_HAIC_label}] Number of samples in one risk group is 0, skip plotting.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), dpi=300)\n",
    "\n",
    "    kmf_high.fit(T_high, event_observed=E_high, label=\"High Risk\")\n",
    "    kmf_low.fit(T_low, event_observed=E_low, label=\"Low Risk\")\n",
    "\n",
    "    ax = kmf_high.plot(ci_show=True,\n",
    "                       color=colors['High'],\n",
    "                       ci_alpha=0.2,\n",
    "                       linewidth=3,\n",
    "                       show_censors=False,\n",
    "                       ax=ax)\n",
    "    ax = kmf_low.plot(ax=ax,\n",
    "                      ci_show=True,\n",
    "                      color=colors['Low'],\n",
    "                      ci_alpha=0.2,\n",
    "                      linewidth=3,\n",
    "                      show_censors=False)\n",
    "\n",
    "    times = [0, 12, 24, 36]\n",
    "    high_risk_counts = [sum(T_high >= t) for t in times]\n",
    "    low_risk_counts = [sum(T_low >= t) for t in times]\n",
    "\n",
    "    from lifelines.utils import ConvergenceWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    lr_result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)\n",
    "\n",
    "    df_cph = df_TACE_HAIC.loc[mask_high | mask_low, [\"time_months\", \"event\", \"risk_flag\"]].dropna()\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(df_cph, duration_col=\"time_months\", event_col=\"event\")\n",
    "    hr = cph.hazard_ratios_[\"risk_flag\"]\n",
    "    ci_lower = cph.summary.loc[\"risk_flag\", \"exp(coef) lower 95%\"]\n",
    "    ci_upper = cph.summary.loc[\"risk_flag\", \"exp(coef) upper 95%\"]\n",
    "    p_value = cph.summary.loc[\"risk_flag\", \"p\"]\n",
    "\n",
    "    if p_value < 0.001:\n",
    "        p_text = \"P < 0.001\"\n",
    "    else:\n",
    "        p_text = f\"P = {p_value:.3f}\"\n",
    "\n",
    "    textstr = f\"HR = {hr:.2f} (95% CI: {ci_lower:.2f}-{ci_upper:.2f})\\n{p_text}\"\n",
    "    plt.text(0.95, 0.95, textstr,\n",
    "             transform=ax.transAxes,\n",
    "             horizontalalignment='right',\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(facecolor='none', edgecolor='none', pad=2.5),\n",
    "             fontsize=30, family=\"Arial\")\n",
    "\n",
    "    plt.title(f\"{dataset_name} ({TACE_HAIC_label})\",\n",
    "              fontsize=30,\n",
    "              pad=20,\n",
    "              family=\"Arial\")\n",
    "    plt.xlabel(\"Months\", fontsize=30, family=\"Arial\")\n",
    "    plt.ylabel(\"Survival Probability\", fontsize=30, family=\"Arial\")\n",
    "\n",
    "    plt.legend(\n",
    "        loc='lower left',\n",
    "        frameon=True,\n",
    "        edgecolor='none',\n",
    "        shadow=False,\n",
    "        fontsize=30,\n",
    "        facecolor='none',\n",
    "        framealpha=0\n",
    "    )\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xticks(times, fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.subplots_adjust(bottom=0.22)\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    for i, t in enumerate(times):\n",
    "        plt.text(t, -0.08, str(t),\n",
    "                 ha='center', va='bottom',\n",
    "                 transform=ax.get_xaxis_transform(),\n",
    "                 fontsize=30, family=\"Arial\")\n",
    "    x_min = min(times)\n",
    "    time_interval = times[1] - times[0]\n",
    "    label_x_pos = x_min - time_interval/4\n",
    "    plt.text(label_x_pos, -0.18, \"High risk\",\n",
    "             ha='right', va='center',\n",
    "             transform=ax.get_xaxis_transform(),\n",
    "             fontsize=30, family=\"Arial\",\n",
    "             bbox=dict(facecolor='none', edgecolor='none', pad=2.5))\n",
    "\n",
    "    plt.text(label_x_pos, -0.26, \"Low risk\",\n",
    "             ha='right', va='center',\n",
    "             transform=ax.get_xaxis_transform(),\n",
    "             fontsize=30, family=\"Arial\",\n",
    "             bbox=dict(facecolor='none', edgecolor='none', pad=2.5))\n",
    "    for i, (high_count, low_count) in enumerate(zip(high_risk_counts, low_risk_counts)):\n",
    "        plt.text(times[i], -0.18, str(high_count),\n",
    "                 ha='center', va='center',\n",
    "                 transform=ax.get_xaxis_transform(),\n",
    "                 fontsize=30, family=\"Arial\")\n",
    "        plt.text(times[i], -0.26, str(low_count),\n",
    "                 ha='center', va='center',\n",
    "                 transform=ax.get_xaxis_transform(),\n",
    "                 fontsize=30, family=\"Arial\")\n",
    "    plt.xlim(times[0], times[-1])\n",
    "    if save_pdf:\n",
    "        pdfname = f\"KM_{dataset_name.replace(' ', '_')}_TACE_HAIC_{TACE_HAIC_label.replace('+', 'plus').replace('/', '_')}.pdf\"\n",
    "        plt.savefig(pdfname, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {pdfname}\")\n",
    "    plt.show()\n",
    "    print(f\"{dataset_name} ({TACE_HAIC_label}) Log-rank Test: P-value = {lr_result.p_value:.4e}\")\n",
    "    print(f\"{dataset_name} ({TACE_HAIC_label}) Cox Regression: HR = {hr:.2f} (95% CI: {ci_lower:.2f}-{ci_upper:.2f}), {p_text}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"=== Training Set (TACE+HAIC) ===\")\n",
    "plot_km_by_TACE_HAIC(df_train, dataset_name=\"Training Set\", T1H2TH0_value=1, median_cut=median_cut)\n",
    "print(\"\\n=== Training Set (TACE/HAIC) ===\")\n",
    "plot_km_by_TACE_HAIC(df_train, dataset_name=\"Training Set\", T1H2TH0_value=0, median_cut=median_cut)\n",
    "print(\"\\n=== Test Set (TACE+HAIC) ===\")\n",
    "plot_km_by_TACE_HAIC(df_test, dataset_name=\"Test Set\", T1H2TH0_value=1, median_cut=median_cut)\n",
    "print(\"\\n=== Test Set (TACE/HAIC) ===\")\n",
    "plot_km_by_TACE_HAIC(df_test, dataset_name=\"Test Set\", T1H2TH0_value=0, median_cut=median_cut)\n",
    "print(\"\\n=== External Validation Set (TACE+HAIC) ===\")\n",
    "plot_km_by_TACE_HAIC(df_ext, dataset_name=\"External Validation Set\", T1H2TH0_value=1, median_cut=median_cut)\n",
    "print(\"\\n=== External Validation Set (TACE/HAIC) ===\")\n",
    "plot_km_by_TACE_HAIC(df_ext, dataset_name=\"External Validation Set\", T1H2TH0_value=0, median_cut=median_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15dcc56c-0802-45d8-86a4-886aecceeba9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP for the late-fusion survival model\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.spatial import KDTree\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Assumes:\n",
    "#   - X_train1, X_test1: imaging data (features)\n",
    "#   - X_train2, X_test2: clinical data (features)\n",
    "#   - y_train, y_test: survival outcome with fields \"fstat\", \"lenfol\"\n",
    "#   - best_svm_model1: imaging model\n",
    "#   - best_svm_model2: clinical model\n",
    "#   - selected_features_mod1: imaging feature names (array-like)\n",
    "#   - selected_features_mod2: clinical feature names (array-like)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def visualize_shap(values, X_data, feature_names, title, output_filename, imaging_prefix=\"RF\", clinical_prefix=None):\n",
    "    \"\"\"\n",
    "    values: SHAP values (n_samples, n_features)\n",
    "    X_data: original feature array (n_samples, n_features)\n",
    "    feature_names: list of feature names (length = n_features)\n",
    "    imaging_prefix: prefix for imaging features, default 'RF'\n",
    "    clinical_prefix: prefix for clinical features, default None (do not rename)\n",
    "    \"\"\"\n",
    "    # Set global font\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "    n_features = len(feature_names)\n",
    "    # Generate display feature names if needed\n",
    "    if imaging_prefix is not None and clinical_prefix is None:\n",
    "        # imaging features: use RF abbreviation, clinical features: original name\n",
    "        # Determine if unimodal or fusion\n",
    "        if n_features == len(selected_features_mod1):\n",
    "            # imaging only\n",
    "            imaging_features_abbr = [f\"{imaging_prefix} {i+1}\" for i in range(n_features)]\n",
    "            pd.DataFrame({\n",
    "                f\"{imaging_prefix} Abbreviation\": imaging_features_abbr,\n",
    "                \"Original Imaging Feature Name\": feature_names\n",
    "            }).to_csv(f\"{output_filename.replace('.pdf', '_rf_abbreviation.csv')}\", index=False)\n",
    "            display_names = imaging_features_abbr\n",
    "            clinical_cut = 0\n",
    "        elif n_features == len(selected_features_mod1) + len(selected_features_mod2):\n",
    "            # fusion\n",
    "            imaging_features_abbr = [f\"{imaging_prefix} {i+1}\" for i in range(len(selected_features_mod1))]\n",
    "            clinical_features_names = list(selected_features_mod2)\n",
    "            pd.DataFrame({\n",
    "                f\"{imaging_prefix} Abbreviation\": imaging_features_abbr,\n",
    "                \"Original Imaging Feature Name\": list(selected_features_mod1)\n",
    "            }).to_csv(f\"{output_filename.replace('.pdf', '_rf_abbreviation.csv')}\", index=False)\n",
    "            display_names = imaging_features_abbr + clinical_features_names\n",
    "            clinical_cut = len(imaging_features_abbr)\n",
    "        else:\n",
    "            # fallback: do not rename\n",
    "            display_names = feature_names\n",
    "            clinical_cut = 0\n",
    "    elif imaging_prefix is None and clinical_prefix is None:\n",
    "        # clinical only\n",
    "        display_names = feature_names\n",
    "        clinical_cut = len(display_names)\n",
    "    else:\n",
    "        display_names = feature_names\n",
    "        clinical_cut = 0\n",
    "\n",
    "    shap_df = pd.DataFrame(values, columns=display_names)\n",
    "    X_df = pd.DataFrame(X_data, columns=display_names)\n",
    "    print(f\"SHAP DataFrame shape: {shap_df.shape}\")\n",
    "\n",
    "    # Set up visualization parameters\n",
    "    jitter_scale = 0.15\n",
    "    distance_threshold = 0.05\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"custom\", [\"#3465A4\", \"#E6E6E6\", \"#CC3333\"])\n",
    "\n",
    "    # Compute mean absolute SHAP values and sort features descending\n",
    "    mean_abs_shap_values = shap_df.abs().mean().sort_values(ascending=False)\n",
    "    sorted_features = mean_abs_shap_values.index\n",
    "\n",
    "    # Build plotting DataFrame with normalized feature values\n",
    "    plot_data = []\n",
    "    for feature in sorted_features:\n",
    "        for shap_val, feat_val in zip(shap_df[feature], X_df[feature]):\n",
    "            norm_val = (feat_val - X_df[feature].min()) / (X_df[feature].max() - X_df[feature].min() + 1e-8)\n",
    "            plot_data.append({\"Feature\": feature, \"SHAP Value\": shap_val, \"Normalized Value\": norm_val})\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Build a KDTree on SHAP values for jittering\n",
    "    all_points = plot_df[\"SHAP Value\"].values.reshape(-1, 1)\n",
    "    tree = KDTree(all_points)\n",
    "\n",
    "    # Color grouping\n",
    "    if n_features == len(selected_features_mod1) + len(selected_features_mod2):\n",
    "        n_imaging = len(selected_features_mod1)\n",
    "        n_clinical = len(selected_features_mod2)\n",
    "    elif n_features == len(selected_features_mod1):\n",
    "        n_imaging = n_features\n",
    "        n_clinical = 0\n",
    "    elif n_features == len(selected_features_mod2):\n",
    "        n_imaging = 0\n",
    "        n_clinical = n_features\n",
    "    else:\n",
    "        n_imaging = n_features // 2\n",
    "        n_clinical = n_features - n_imaging\n",
    "\n",
    "    feature_groups = {\n",
    "        \"Imaging\": display_names[:n_imaging],\n",
    "        \"Clinical\": display_names[n_imaging:]\n",
    "    }\n",
    "    group_colors = {\"Imaging\": \"#6A9ACF\", \"Clinical\": \"#ABDAEC\"}\n",
    "    feature_colors = []\n",
    "    for feature in sorted_features:\n",
    "        assigned = \"#cccccc\"\n",
    "        for group, feats in feature_groups.items():\n",
    "            if feature in feats:\n",
    "                assigned = group_colors.get(group, \"#000000\")\n",
    "                break\n",
    "        feature_colors.append(assigned)\n",
    "\n",
    "    # Set up figure and grid layout, more compact color bar\n",
    "    fig = plt.figure(figsize=(22, 12))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[0.85, 0.15], height_ratios=[0.05, 0.95])\n",
    "    ax = plt.subplot(gs[1, 0])\n",
    "\n",
    "    # Scatter plot: each feature's SHAP values with jitter on y-axis\n",
    "    for i, feature in enumerate(sorted_features):\n",
    "        subset = plot_df[plot_df[\"Feature\"] == feature]\n",
    "        shap_vals = subset[\"SHAP Value\"].values\n",
    "        jit = np.zeros(len(shap_vals))\n",
    "        for idx, val in enumerate(shap_vals):\n",
    "            if len(tree.query_ball_point([val], r=distance_threshold)) > 1:\n",
    "                jit[idx] = np.random.normal(0, jitter_scale)\n",
    "        ax.scatter(subset[\"SHAP Value\"], jit + i, c=subset[\"Normalized Value\"],\n",
    "                   cmap=custom_cmap, s=25, alpha=0.7)\n",
    "\n",
    "    ax.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.5)\n",
    "    ax.tick_params(axis='x', labelsize=20)\n",
    "    # ax.set_xlabel(\"SHAP Value (Impact on Risk Score)\", fontsize=20, family=\"Arial\")\n",
    "    # ax.set_ylabel(\"Features\", fontsize=20, family=\"Arial\")\n",
    "    ax.set_yticks(range(len(sorted_features)))\n",
    "    ax.set_yticklabels(sorted_features, fontsize=28, family=\"Arial\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(False)\n",
    "    plt.xticks(fontsize=30)\n",
    "    # Bar chart showing mean(|SHAP value|)\n",
    "    ax_bar = plt.subplot(gs[1, 1], sharey=ax)\n",
    "    bars = ax_bar.barh(range(len(sorted_features)), mean_abs_shap_values.values,\n",
    "                       color=feature_colors, alpha=0.7)\n",
    "    ax_bar.set_xlim(0, mean_abs_shap_values.max() * 1.7)\n",
    "    for bar, value in zip(bars, mean_abs_shap_values.values):\n",
    "        ax_bar.text(value + mean_abs_shap_values.max() * 0.005,\n",
    "                    bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{value:.3f}\", va=\"center\", fontsize=30, color=\"black\", family=\"Arial\")\n",
    "    ax_bar.tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\n",
    "    ax_bar.grid(False)\n",
    "    plt.xticks(fontsize=30)\n",
    "    # Colorbar for scatter plot, compact without any text\n",
    "    cax = plt.subplot(gs[0, 0])\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cax, orientation=\"horizontal\")\n",
    "    cbar.outline.set_visible(False)\n",
    "    cbar.ax.tick_params(labelsize=16)\n",
    "    cbar.ax.xaxis.set_ticks([])\n",
    "    # Remove colorbar title and min/max labels\n",
    "    cbar.ax.set_title(\"\")\n",
    "    cbar.ax.set_xticklabels([])\n",
    "    # Do not display low/high labels\n",
    "    plt.subplots_adjust(top=0.995, wspace=0.05)\n",
    "    # plt.suptitle(title, fontsize=24, family=\"Arial\")\n",
    "    plt.savefig(output_filename, format=\"pdf\", bbox_inches=\"tight\", dpi=1200)\n",
    "    plt.show()\n",
    "    print(f\"{title} SHAP visualization finished and saved to {output_filename}\")\n",
    "\n",
    "\n",
    "# ========== SHAP for Fusion Survival Model (Imaging + Clinical) ==========\n",
    "d1 = X_test1.shape[1]\n",
    "d2 = X_test2.shape[1]\n",
    "X_fusion_test = np.concatenate((X_test1, X_test2), axis=1)\n",
    "fusion_feature_names_abbr = [f\"RF {i+1}\" for i in range(len(selected_features_mod1))] + list(selected_features_mod2)\n",
    "pd.DataFrame({\n",
    "    \"RF Abbreviation\": [f\"RF {i+1}\" for i in range(len(selected_features_mod1))],\n",
    "    \"Original Imaging Feature Name\": list(selected_features_mod1)\n",
    "}).to_csv(\"survival_shap_fusion_rf_abbreviation.csv\", index=False)\n",
    "\n",
    "# Compute weights using training set predictions\n",
    "pred1_train = best_svm_model1.predict(X_train1)\n",
    "pred2_train = best_svm_model2.predict(X_train2)\n",
    "cindex1 = concordance_index_censored(y_train[\"fstat\"], y_train[\"lenfol\"], pred1_train)[0]\n",
    "cindex2 = concordance_index_censored(y_train[\"fstat\"], y_train[\"lenfol\"], pred2_train)[0]\n",
    "tau = 0.1\n",
    "w1 = np.exp((cindex1 - 0.5) / tau)\n",
    "w2 = np.exp((cindex2 - 0.5) / tau)\n",
    "total = w1 + w2\n",
    "norm_w1 = w1 / total\n",
    "norm_w2 = w2 / total\n",
    "print(f\"Fusion weights: {norm_w1:.3f} (imaging), {norm_w2:.3f} (clinical)\")\n",
    "\n",
    "def fusion_model_wrapper(X):\n",
    "    X1_part = X[:, :d1]\n",
    "    X2_part = X[:, d1:]\n",
    "    risk1 = best_svm_model1.predict(X1_part)\n",
    "    risk2 = best_svm_model2.predict(X2_part)\n",
    "    return norm_w1 * risk1 + norm_w2 * risk2\n",
    "\n",
    "explainer_fusion = shap.Explainer(fusion_model_wrapper, X_fusion_test)\n",
    "shap_values_fusion_obj = explainer_fusion(X_fusion_test)\n",
    "shap_values_fusion = shap_values_fusion_obj.values\n",
    "print(\"Fusion model SHAP values shape:\", shap_values_fusion.shape)\n",
    "visualize_shap(shap_values_fusion, X_fusion_test, fusion_feature_names_abbr,\n",
    "               \"\", \"survival_shap_fusion.pdf\", imaging_prefix=\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac9ddca0-b6b2-4aa6-a3cb-e6ca0e3420a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Performence AUC-T\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create DataFrame, RC changed to Late Fusion\n",
    "data = {\n",
    "    'Model': ['Radiomics', 'Radiomics', 'Radiomics', 'Radiomics',\n",
    "              'Clinical', 'Clinical', 'Clinical', 'Clinical',\n",
    "              'LFICKI-OS', 'LFICKI-OS', 'LFICKI-OS', 'LFICKI-OS'] * 3,\n",
    "    'Time': [6, 12, 24, 36] * 9,\n",
    "    'AUC': [0.78, 0.83, 0.75, 0.70,  # Radiomics Training\n",
    "            0.87, 0.82, 0.85, 0.83,  # Clinical Training\n",
    "            0.91, 0.88, 0.86, 0.82,  # Late Fusion Training\n",
    "            0.83, 0.90, 0.70, 0.73,  # Radiomics Testing\n",
    "            0.91, 0.88, 0.66, 0.61,  # Clinical Testing\n",
    "            0.87, 0.93, 0.73, 0.73,  # Late Fusion Testing\n",
    "            0.78, 0.70, 0.74, 0.73,  # Radiomics External\n",
    "            0.75, 0.70, 0.68, 0.66,  # Clinical External\n",
    "            0.88, 0.77, 0.76, 0.74], # Late Fusion External\n",
    "    'Cohort': ['Training'] * 12 + ['Testing'] * 12 + ['External Validation'] * 12\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set global font to Arial\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"legend.fontsize\"] = 24\n",
    "plt.rcParams[\"figure.titlesize\"] = 22\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Orange gradient color map\n",
    "color_map = {\n",
    "    'Radiomics': '#FFD580',      # Light orange\n",
    "    'Clinical': '#FFA040',       # Orange\n",
    "    'LFICKI-OS': '#FF6F00'     # Dark orange\n",
    "}\n",
    "marker_map = {\n",
    "    'Radiomics': '-o',\n",
    "    'Clinical': '-s',\n",
    "    'LFICKI-OS': '-^'\n",
    "}\n",
    "\n",
    "# Create three subplots\n",
    "for i, (ax, cohort) in enumerate(zip(axes, ['Training', 'Testing', 'External Validation'])):\n",
    "    for model in ['Radiomics', 'Clinical', 'LFICKI-OS']:\n",
    "        style = marker_map[model]\n",
    "        data_sub = df[(df['Cohort'] == cohort) & (df['Model'] == model)]\n",
    "        ax.plot(data_sub['Time'], data_sub['AUC'], style, label=model, color=color_map[model])\n",
    "\n",
    "    ax.set_title(f'{cohort} Cohort', fontsize=30, fontfamily=\"Arial\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0.5, 1.0)\n",
    "    ax.set_xticks([6, 12, 24, 36])\n",
    "    ax.set_xticklabels([6, 12, 24, 36], fontsize=30, fontfamily=\"Arial\")\n",
    "    ax.set_yticks([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('AUC', fontsize=30, fontfamily=\"Arial\")\n",
    "        ax.legend(loc='lower left', fontsize=30, frameon=False)\n",
    "        ax.tick_params(axis='y', labelsize=30)  # Keep left y-axis ticks\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "        ax.tick_params(axis='y', labelleft=False)  # Hide right y-axis ticks\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_xlabel('Follow-up Time (Months)', fontsize=30, fontfamily=\"Arial\")\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"survival_auc_t_three_modalities.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6a312e-6022-4d3a-aec0-62f45cf985fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performence Brier score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create DataFrame, RC changed to Late Fusion\n",
    "data = {\n",
    "    'Model': ['Radiomics', 'Radiomics', 'Radiomics', 'Radiomics',\n",
    "              'Clinical', 'Clinical', 'Clinical', 'Clinical',\n",
    "              'LFICKI-OS', 'LFICKI-OS', 'LFICKI-OS', 'LFICKI-OS'] * 3,\n",
    "    'Time': [6, 12, 24, 36] * 9,\n",
    "    'Brier': [0.24, 0.22, 0.22, 0.23,  # Radiomics Training\n",
    "              0.23, 0.22, 0.20, 0.21,  # Clinical Training\n",
    "              0.23, 0.22, 0.21, 0.21,  # Late Fusion Training\n",
    "              0.22, 0.20, 0.22, 0.22,  # Radiomics Testing\n",
    "              0.23, 0.22, 0.24, 0.24,  # Clinical Testing\n",
    "              0.22, 0.21, 0.23, 0.23,  # Late Fusion Testing\n",
    "              0.23, 0.22, 0.23, 0.23,  # Radiomics External\n",
    "              0.24, 0.23, 0.23, 0.24,  # Clinical External\n",
    "              0.23, 0.23, 0.23, 0.23], # Late Fusion External\n",
    "    'Cohort': ['Training'] * 12 + ['Testing'] * 12 + ['External Validation'] * 12\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Integrated Brier Scores, RC changed to Late Fusion\n",
    "ibs = {\n",
    "    'Training': {'Radiomics': 0.23, 'Clinical': 0.21, 'LFICKI-OS': 0.22},\n",
    "    'Testing': {'Radiomics': 0.21, 'Clinical': 0.23, 'LFICKI-OS': 0.22},\n",
    "    'External Validation': {'Radiomics': 0.23, 'Clinical': 0.23, 'LFICKI-OS': 0.23}\n",
    "}\n",
    "\n",
    "# Set global font to Arial\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "plt.rcParams[\"figure.titlesize\"] = 22\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "color_map = {\n",
    "    'Radiomics': 'lightblue',\n",
    "    'Clinical': 'blue',\n",
    "    'LFICKI-OS': 'darkblue'\n",
    "}\n",
    "marker_map = {\n",
    "    'Radiomics': '-o',\n",
    "    'Clinical': '-s',\n",
    "    'LFICKI-OS': '-^'\n",
    "}\n",
    "\n",
    "for i, (ax, cohort) in enumerate(zip(axes, ['Training', 'Testing', 'External Validation'])):\n",
    "    for model in ['Radiomics', 'Clinical', 'LFICKI-OS']:\n",
    "        style = marker_map[model]\n",
    "        data_sub = df[(df['Cohort'] == cohort) & (df['Model'] == model)]\n",
    "        color = color_map[model]\n",
    "        # Plot Brier Score over time\n",
    "        ax.plot(data_sub['Time'], data_sub['Brier'], style, \n",
    "                label=f\"{model}\",\n",
    "                color=color)\n",
    "        # Plot Integrated Brier Score as horizontal line\n",
    "        ax.axhline(y=ibs[cohort][model], color=color, linestyle='--', \n",
    "                   linewidth=2, alpha=0.5)\n",
    "\n",
    "    ax.set_title(f'{cohort} Cohort', fontsize=30, fontfamily=\"Arial\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0.15, 0.30)\n",
    "    ax.set_xticks([6, 12, 24, 36])\n",
    "    ax.set_xticklabels([6, 12, 24, 36], fontsize=30, fontfamily=\"Arial\")\n",
    "    ax.set_yticks([0.15, 0.18, 0.21, 0.24, 0.27, 0.30])\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Brier Score', fontsize=30, fontfamily=\"Arial\")\n",
    "        ax.legend(loc='upper left',  fontsize=24, frameon=False)\n",
    "        ax.tick_params(axis='y', labelsize=30)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "        ax.tick_params(axis='y', labelleft=False)  # Hide non-left y-axis ticks\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_xlabel('Follow-up Time (Months)', fontsize=30, fontfamily=\"Arial\")\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"survival_brier_ibs_comparison.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68adec9-f378-4242-81c4-930961e8d384",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Performence C-index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create DataFrame, CR changed to Late Fusion\n",
    "data = {\n",
    "    'Modality': ['Radiomics', 'Clinical', 'LFICKI-OS'],\n",
    "    'Training': [0.75, 0.79, 0.82],\n",
    "    'Training_CI_Lower': [0.73, 0.76, 0.81],\n",
    "    'Training_CI_Upper': [0.76, 0.81, 0.83],\n",
    "    'Testing': [0.72, 0.69, 0.76],\n",
    "    'Testing_CI_Lower': [0.69, 0.66, 0.73],\n",
    "    'Testing_CI_Upper': [0.74, 0.73, 0.79],\n",
    "    'External_Validation': [0.66, 0.68, 0.72]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set global font to Arial\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"legend.fontsize\"] = 24\n",
    "plt.rcParams[\"figure.titlesize\"] = 22\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Set background style\n",
    "sns.set_style(\"white\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Set soft color scheme\n",
    "colors = ['#8ecae6', '#219ebc', '#126782']\n",
    "\n",
    "# Set group positions\n",
    "x = np.arange(len(data['Modality']))\n",
    "width = 0.25\n",
    "\n",
    "# Calculate error bar values\n",
    "training_yerr = np.array([\n",
    "    np.array(data['Training']) - np.array(data['Training_CI_Lower']),\n",
    "    np.array(data['Training_CI_Upper']) - np.array(data['Training'])\n",
    "])\n",
    "testing_yerr = np.array([\n",
    "    np.array(data['Testing']) - np.array(data['Testing_CI_Lower']),\n",
    "    np.array(data['Testing_CI_Upper']) - np.array(data['Testing'])\n",
    "])\n",
    "\n",
    "# Plot bars and error bars\n",
    "training_bars = ax.bar(x - width, data['Training'], width, label='Training',\n",
    "                      color=colors[0], yerr=training_yerr, capsize=5)\n",
    "testing_bars = ax.bar(x, data['Testing'], width, label='Testing',\n",
    "                     color=colors[1], yerr=testing_yerr, capsize=5)\n",
    "external_bars = ax.bar(x + width, data['External_Validation'], width,\n",
    "                      label='External Validation', color=colors[2])\n",
    "\n",
    "# Add value labels\n",
    "def add_value_labels(bars, fontsize=24):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontsize=fontsize, fontfamily=\"Arial\")\n",
    "\n",
    "add_value_labels(training_bars)\n",
    "add_value_labels(testing_bars)\n",
    "add_value_labels(external_bars)\n",
    "\n",
    "# Customize chart\n",
    "# ax.set_xlabel('Model', fontsize=18, fontfamily=\"Arial\")\n",
    "ax.set_ylabel('C-index', fontsize=24, fontfamily=\"Arial\")\n",
    "# ax.set_title('C-index Comparison Across Different Models',\n",
    "#              fontsize=22, pad=20, fontfamily=\"Arial\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(data['Modality'], fontsize=24, fontfamily=\"Arial\")\n",
    "ax.legend(fontsize=24, frameon=False)\n",
    "plt.yticks(fontsize=24)\n",
    "# Calculate y-axis limit\n",
    "all_values = []\n",
    "all_values.extend(data['Training'])\n",
    "all_values.extend(data['Testing'])\n",
    "all_values.extend(data['External_Validation'])\n",
    "all_values.extend(data['Training_CI_Upper'])\n",
    "all_values.extend(data['Testing_CI_Upper'])\n",
    "\n",
    "ymin = 0.5\n",
    "ymax = max(all_values) + 0.05\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save as PDF\n",
    "plt.savefig(\"survival_three_modalities_comparison.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6b5683-ed20-4da0-9f9e-8cd4ab30cc27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Performence model selection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data section, model type names in English, keep Early fusion as is\n",
    "data = {\n",
    "    'Model_Type': ['Radiomics']*6 + ['Clinical']*6 + ['Early fusion']*6,\n",
    "    'Algorithm': ['RSF', 'GBSA', 'FastSurvivalSVM']*6,\n",
    "    'Dataset': ['Test', 'Test', 'Test', 'Training', 'Training', 'Training']*3,\n",
    "    'C-index': [0.63, 0.65, 0.72, 0.82, 0.83, 0.73,\n",
    "                0.61, 0.60, 0.69, 0.80, 0.83, 0.77,\n",
    "                0.61, 0.60, 0.69, 0.80, 0.83, 0.77],\n",
    "    'CI_Lower': [0.60, 0.62, 0.69, 0.80, 0.80, 0.72,\n",
    "                 0.56, 0.56, 0.66, 0.79, 0.81, 0.77,\n",
    "                 0.56, 0.56, 0.66, 0.79, 0.81, 0.77],\n",
    "    'CI_Upper': [0.66, 0.68, 0.74, 0.83, 0.85, 0.74,\n",
    "                 0.65, 0.63, 0.73, 0.82, 0.84, 0.78,\n",
    "                 0.65, 0.63, 0.73, 0.82, 0.84, 0.78]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set global font to Arial, font size consistent with previous plots\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"legend.fontsize\"] = 24\n",
    "plt.rcParams[\"figure.titlesize\"] = 22\n",
    "\n",
    "plt.figure(figsize=(22, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "colors = ['#8ecae6', '#219ebc']\n",
    "n_algorithms = 3\n",
    "width = 0.4\n",
    "\n",
    "for i, model_type in enumerate(['Radiomics', 'Clinical', 'Early fusion']):\n",
    "    train_data = df[(df['Model_Type'] == model_type) & (df['Dataset'] == 'Training')]\n",
    "    test_data = df[(df['Model_Type'] == model_type) & (df['Dataset'] == 'Test')]\n",
    "    x = np.arange(n_algorithms) + i * (n_algorithms + 0.2)\n",
    "    # Training set\n",
    "    train_bars = ax.bar(x - width/2, train_data['C-index'], width, \n",
    "                       label='Training' if i == 0 else \"\", color=colors[0])\n",
    "    # Test set\n",
    "    test_bars = ax.bar(x + width/2, test_data['C-index'], width,\n",
    "                      label='Test' if i == 0 else \"\", color=colors[1])\n",
    "    # Training error bars\n",
    "    ax.errorbar(x - width/2, train_data['C-index'],\n",
    "               yerr=[train_data['C-index'] - train_data['CI_Lower'],\n",
    "                     train_data['CI_Upper'] - train_data['C-index']],\n",
    "               fmt='none', color='black', capsize=5, alpha=0.6, elinewidth=1.1, capthick=1.1)\n",
    "    # Test error bars\n",
    "    ax.errorbar(x + width/2, test_data['C-index'],\n",
    "               yerr=[test_data['C-index'] - test_data['CI_Lower'],\n",
    "                     test_data['CI_Upper'] - test_data['C-index']],\n",
    "               fmt='none', color='black', capsize=5, alpha=0.6, elinewidth=1.1, capthick=1.1)\n",
    "    # Value labels\n",
    "    for bars in [train_bars, test_bars]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}', ha='center', va='bottom', fontsize=24, fontfamily=\"Arial\")\n",
    "    # Top model type label\n",
    "    plt.text((i * (n_algorithms + 0.2) + 1), 0.90, model_type,\n",
    "             ha='center', va='bottom', fontsize=24, fontweight='bold', fontfamily=\"Arial\")\n",
    "    # Vertical separation dashed line\n",
    "    if i < 2:\n",
    "        separation_x = x[-1] + width + 0.2\n",
    "        plt.axvline(x=separation_x, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Set x-axis labels\n",
    "all_x = np.array([np.arange(n_algorithms) + i * (n_algorithms + 0.2) for i in range(3)]).flatten()\n",
    "plt.xticks(all_x, ['RSF', 'GBSA', 'FSVM'] * 3, fontfamily=\"Arial\", fontsize=24)\n",
    "\n",
    "# plt.xlabel('Algorithm', fontsize=18, labelpad=10, fontfamily=\"Arial\")\n",
    "plt.ylabel('C-index', fontsize=20, fontfamily=\"Arial\")\n",
    "plt.yticks(fontsize=24)\n",
    "# plt.title('C-index Comparison Across Different Models',\n",
    "#           fontsize=22, pad=20, fontfamily=\"Arial\")\n",
    "plt.legend(loc='upper left', fontsize=24, frameon=False)\n",
    "\n",
    "plt.ylim(0.2, 1.05)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.xlim(all_x[0] - width, all_x[-1] + width)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save as PDF\n",
    "plt.savefig(\"survival_multimodal_comparison.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f8ff3e-f3c6-4963-a869-eb37993a7ada",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_abbreviated_feature_names(radiomic_features, clinical_features):\n",
    "    \"\"\"\n",
    "    Automatically abbreviate radiomic features as RF 1,2,...,\n",
    "    keep clinical feature names unchanged, and return a mapping DataFrame.\n",
    "    \"\"\"\n",
    "    radiomic_features = list(radiomic_features)\n",
    "    clinical_features = list(clinical_features)\n",
    "    radiomic_short = [f\"RF {i+1}\" for i in range(len(radiomic_features))]\n",
    "    feature_name_abbr = radiomic_short + clinical_features\n",
    "    abbr_df = pd.DataFrame({\n",
    "        \"Radiomic Abbreviation\": radiomic_short,\n",
    "        \"Original Radiomic Feature Name\": radiomic_features\n",
    "    })\n",
    "    return feature_name_abbr, abbr_df, radiomic_short\n",
    "\n",
    "def plot_fusion_correlation_matrix(X1_train_s, X2_train_s, selected_features_mod1, selected_features_mod2):\n",
    "    \"\"\"\n",
    "    Plot the correlation matrix for the two modalities in the fusion model.\n",
    "    Radiomic features are automatically abbreviated as RF, Arial font, larger font size,\n",
    "    only keep the last row and first column axis labels.\n",
    "    \"\"\"\n",
    "    # Set global font\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "    # Abbreviate feature names: radiomics as RF, clinical features as is\n",
    "    all_features, radiomic_df, radiomic_short = create_abbreviated_feature_names(\n",
    "        selected_features_mod1, selected_features_mod2\n",
    "    )\n",
    "    # Save mapping table\n",
    "    radiomic_df.to_csv(\"fusion_radiomic_feature_abbreviation.csv\", index=False)\n",
    "\n",
    "    # Concatenate data\n",
    "    df = pd.DataFrame(np.hstack([X1_train_s, X2_train_s]),\n",
    "                     columns=all_features)\n",
    "    n = len(df.columns)\n",
    "    \n",
    "    # Calculate correlation matrix and p-values\n",
    "    corr = df.corr()\n",
    "    p_values = pd.DataFrame(np.zeros((n, n)), columns=df.columns, index=df.columns)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                _, p = pearsonr(df.iloc[:, i], df.iloc[:, j])\n",
    "                p_values.iloc[i, j] = p\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n, n, figsize=(2.3 * n, 2.3 * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax = axes[i, j]\n",
    "            if i == j:\n",
    "                # Diagonal: plot histogram\n",
    "                sns.histplot(df.iloc[:, i], kde=True, ax=ax)\n",
    "                ax.set_title(df.columns[i], fontsize=18, pad=6, family=\"Arial\")\n",
    "            elif i > j:\n",
    "                # Lower triangle: plot scatter plot with fit line and confidence interval\n",
    "                sns.scatterplot(x=df.iloc[:, j], y=df.iloc[:, i], ax=ax, edgecolor=None)\n",
    "                X = sm.add_constant(df.iloc[:, j])\n",
    "                model = sm.OLS(df.iloc[:, i], X).fit()\n",
    "                x_values = np.linspace(df.iloc[:, j].min(), df.iloc[:, j].max(), 100)\n",
    "                X_pred = sm.add_constant(x_values)\n",
    "                y_pred = model.predict(X_pred)\n",
    "                conf_int_pred = model.get_prediction(X_pred).conf_int()\n",
    "                ax.plot(x_values, y_pred, color=\"red\")\n",
    "                ax.fill_between(x_values, conf_int_pred[:, 0], conf_int_pred[:, 1], \n",
    "                              color=\"blue\", alpha=0.2)\n",
    "            else:\n",
    "                # Upper triangle: plot heatmap and significance annotation\n",
    "                sns.heatmap(pd.DataFrame([[corr.iloc[i, j]]]), \n",
    "                          cmap=sns.diverging_palette(240, 10, as_cmap=True),\n",
    "                          cbar=False, annot=True, fmt=\".2f\", square=True, \n",
    "                          ax=ax, vmin=-1, vmax=1,\n",
    "                          annot_kws={\"size\": 22, \"family\": \"Arial\"})\n",
    "                # Significance annotation\n",
    "                p_val = p_values.iloc[i, j]\n",
    "                if p_val < 0.001:\n",
    "                    ax.text(0.5, 0.8, \"***\", color=\"black\", ha=\"center\", \n",
    "                           va=\"center\", transform=ax.transAxes, fontsize=16, family=\"Arial\")\n",
    "                elif p_val < 0.01:\n",
    "                    ax.text(0.5, 0.8, \"**\", color=\"black\", ha=\"center\", \n",
    "                           va=\"center\", transform=ax.transAxes, fontsize=16, family=\"Arial\")\n",
    "                elif p_val < 0.05:\n",
    "                    ax.text(0.5, 0.8, \"*\", color=\"black\", ha=\"center\", \n",
    "                           va=\"center\", transform=ax.transAxes, fontsize=16, family=\"Arial\")\n",
    "\n",
    "            # Set axis labels: only keep last row and first column\n",
    "            if i < n - 1:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xticks([0.5])\n",
    "                ax.set_xticklabels([df.columns[j]], fontsize=16, family=\"Arial\", rotation=60, ha=\"right\")\n",
    "            if j > 0:\n",
    "                ax.set_yticks([])\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                ax.set_yticks([0.5])\n",
    "                ax.set_yticklabels([df.columns[i]], fontsize=16, family=\"Arial\", rotation=0, va=\"center\")\n",
    "\n",
    "            # Adjust diagonal subplot title position\n",
    "            if i == j:\n",
    "                ax.set_title(df.columns[i], fontsize=18, pad=6, family=\"Arial\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.subplots_adjust(right=0.86)\n",
    "    cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7])\n",
    "    norm = plt.Normalize(vmin=-1, vmax=1)\n",
    "    sm_map = plt.cm.ScalarMappable(cmap=sns.diverging_palette(240, 10, as_cmap=True), \n",
    "                                  norm=norm)\n",
    "    sm_map.set_array([])\n",
    "    cbar = fig.colorbar(sm_map, cax=cbar_ax)\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    cbar.set_label(\"Pearson correlation\", fontsize=20, family=\"Arial\")\n",
    "\n",
    "    # Add title\n",
    "    plt.suptitle('Fusion Model Features Correlation Matrix', y=0.95, fontsize=20, family=\"Arial\")\n",
    "\n",
    "    # Save and show figure\n",
    "    plt.savefig(\"Fusion_Features_Correlation_Matrix.pdf\", format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Main routine\n",
    "if __name__ == \"__main__\":\n",
    "    print_session_info()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nPlotting fusion feature correlation matrix...\")\n",
    "        print(f\"Number of radiomic features: {len(selected_features_mod1)}\")\n",
    "        print(f\"Number of clinical features: {len(selected_features_mod2)}\")\n",
    "        print(\"\\nRadiomic feature names:\")\n",
    "        for i, feat in enumerate(selected_features_mod1, 1):\n",
    "            print(f\"{i}. {feat}\")\n",
    "        print(\"\\nClinical feature names:\")\n",
    "        for i, feat in enumerate(selected_features_mod2, 1):\n",
    "            print(f\"{i}. {feat}\")\n",
    "            \n",
    "        plot_fusion_correlation_matrix(\n",
    "            X_train1,  # Preprocessed radiomic data\n",
    "            X_train2,  # Preprocessed clinical data\n",
    "            selected_features_mod1,  # Selected radiomic feature names\n",
    "            selected_features_mod2   # Selected clinical feature names\n",
    "        )\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Please ensure the main routine has been run and the necessary variables have been generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "py39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
